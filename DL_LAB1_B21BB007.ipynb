{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "2jatbx8QZLIY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "num_classes = 7\n",
        "input = 16\n",
        "hidden = 100\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "5CDD40pvgomd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/Dry_Bean_Dataset.xlsx\")"
      ],
      "metadata": {
        "id": "Oqazk9hijWVS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "GUMIhcfEBxrG",
        "outputId": "fbacfad9-51a1-49f5-9f97-ab37ac02fb17"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
              "0      28395    610.291       208.178117       173.888747      1.197191   \n",
              "1      28734    638.018       200.524796       182.734419      1.097356   \n",
              "2      29380    624.110       212.826130       175.931143      1.209713   \n",
              "3      30008    645.884       210.557999       182.516516      1.153638   \n",
              "4      30140    620.134       201.847882       190.279279      1.060798   \n",
              "...      ...        ...              ...              ...           ...   \n",
              "13606  42097    759.696       288.721612       185.944705      1.552728   \n",
              "13607  42101    757.499       281.576392       190.713136      1.476439   \n",
              "13608  42139    759.321       281.539928       191.187979      1.472582   \n",
              "13609  42147    763.779       283.382636       190.275731      1.489326   \n",
              "13610  42159    772.237       295.142741       182.204716      1.619841   \n",
              "\n",
              "       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
              "0          0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
              "1          0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
              "2          0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
              "3          0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
              "4          0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
              "...             ...         ...            ...       ...       ...        ...   \n",
              "13606      0.765002       42508     231.515799  0.714574  0.990331   0.916603   \n",
              "13607      0.735702       42494     231.526798  0.799943  0.990752   0.922015   \n",
              "13608      0.734065       42569     231.631261  0.729932  0.989899   0.918424   \n",
              "13609      0.741055       42667     231.653248  0.705389  0.987813   0.907906   \n",
              "13610      0.786693       42600     231.686223  0.788962  0.989648   0.888380   \n",
              "\n",
              "       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
              "0         0.913358      0.007332      0.003147      0.834222      0.998724   \n",
              "1         0.953861      0.006979      0.003564      0.909851      0.998430   \n",
              "2         0.908774      0.007244      0.003048      0.825871      0.999066   \n",
              "3         0.928329      0.007017      0.003215      0.861794      0.994199   \n",
              "4         0.970516      0.006697      0.003665      0.941900      0.999166   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "13606     0.801865      0.006858      0.001749      0.642988      0.998385   \n",
              "13607     0.822252      0.006688      0.001886      0.676099      0.998219   \n",
              "13608     0.822730      0.006681      0.001888      0.676884      0.996767   \n",
              "13609     0.817457      0.006724      0.001852      0.668237      0.995222   \n",
              "13610     0.784997      0.007001      0.001640      0.616221      0.998180   \n",
              "\n",
              "          Class  \n",
              "0         SEKER  \n",
              "1         SEKER  \n",
              "2         SEKER  \n",
              "3         SEKER  \n",
              "4         SEKER  \n",
              "...         ...  \n",
              "13606  DERMASON  \n",
              "13607  DERMASON  \n",
              "13608  DERMASON  \n",
              "13609  DERMASON  \n",
              "13610  DERMASON  \n",
              "\n",
              "[13611 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c0717b9-db48-4740-8450-e1075e505e9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Perimeter</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>AspectRation</th>\n",
              "      <th>Eccentricity</th>\n",
              "      <th>ConvexArea</th>\n",
              "      <th>EquivDiameter</th>\n",
              "      <th>Extent</th>\n",
              "      <th>Solidity</th>\n",
              "      <th>roundness</th>\n",
              "      <th>Compactness</th>\n",
              "      <th>ShapeFactor1</th>\n",
              "      <th>ShapeFactor2</th>\n",
              "      <th>ShapeFactor3</th>\n",
              "      <th>ShapeFactor4</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28395</td>\n",
              "      <td>610.291</td>\n",
              "      <td>208.178117</td>\n",
              "      <td>173.888747</td>\n",
              "      <td>1.197191</td>\n",
              "      <td>0.549812</td>\n",
              "      <td>28715</td>\n",
              "      <td>190.141097</td>\n",
              "      <td>0.763923</td>\n",
              "      <td>0.988856</td>\n",
              "      <td>0.958027</td>\n",
              "      <td>0.913358</td>\n",
              "      <td>0.007332</td>\n",
              "      <td>0.003147</td>\n",
              "      <td>0.834222</td>\n",
              "      <td>0.998724</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28734</td>\n",
              "      <td>638.018</td>\n",
              "      <td>200.524796</td>\n",
              "      <td>182.734419</td>\n",
              "      <td>1.097356</td>\n",
              "      <td>0.411785</td>\n",
              "      <td>29172</td>\n",
              "      <td>191.272750</td>\n",
              "      <td>0.783968</td>\n",
              "      <td>0.984986</td>\n",
              "      <td>0.887034</td>\n",
              "      <td>0.953861</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>0.909851</td>\n",
              "      <td>0.998430</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29380</td>\n",
              "      <td>624.110</td>\n",
              "      <td>212.826130</td>\n",
              "      <td>175.931143</td>\n",
              "      <td>1.209713</td>\n",
              "      <td>0.562727</td>\n",
              "      <td>29690</td>\n",
              "      <td>193.410904</td>\n",
              "      <td>0.778113</td>\n",
              "      <td>0.989559</td>\n",
              "      <td>0.947849</td>\n",
              "      <td>0.908774</td>\n",
              "      <td>0.007244</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>0.825871</td>\n",
              "      <td>0.999066</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30008</td>\n",
              "      <td>645.884</td>\n",
              "      <td>210.557999</td>\n",
              "      <td>182.516516</td>\n",
              "      <td>1.153638</td>\n",
              "      <td>0.498616</td>\n",
              "      <td>30724</td>\n",
              "      <td>195.467062</td>\n",
              "      <td>0.782681</td>\n",
              "      <td>0.976696</td>\n",
              "      <td>0.903936</td>\n",
              "      <td>0.928329</td>\n",
              "      <td>0.007017</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.861794</td>\n",
              "      <td>0.994199</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30140</td>\n",
              "      <td>620.134</td>\n",
              "      <td>201.847882</td>\n",
              "      <td>190.279279</td>\n",
              "      <td>1.060798</td>\n",
              "      <td>0.333680</td>\n",
              "      <td>30417</td>\n",
              "      <td>195.896503</td>\n",
              "      <td>0.773098</td>\n",
              "      <td>0.990893</td>\n",
              "      <td>0.984877</td>\n",
              "      <td>0.970516</td>\n",
              "      <td>0.006697</td>\n",
              "      <td>0.003665</td>\n",
              "      <td>0.941900</td>\n",
              "      <td>0.999166</td>\n",
              "      <td>SEKER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13606</th>\n",
              "      <td>42097</td>\n",
              "      <td>759.696</td>\n",
              "      <td>288.721612</td>\n",
              "      <td>185.944705</td>\n",
              "      <td>1.552728</td>\n",
              "      <td>0.765002</td>\n",
              "      <td>42508</td>\n",
              "      <td>231.515799</td>\n",
              "      <td>0.714574</td>\n",
              "      <td>0.990331</td>\n",
              "      <td>0.916603</td>\n",
              "      <td>0.801865</td>\n",
              "      <td>0.006858</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.642988</td>\n",
              "      <td>0.998385</td>\n",
              "      <td>DERMASON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13607</th>\n",
              "      <td>42101</td>\n",
              "      <td>757.499</td>\n",
              "      <td>281.576392</td>\n",
              "      <td>190.713136</td>\n",
              "      <td>1.476439</td>\n",
              "      <td>0.735702</td>\n",
              "      <td>42494</td>\n",
              "      <td>231.526798</td>\n",
              "      <td>0.799943</td>\n",
              "      <td>0.990752</td>\n",
              "      <td>0.922015</td>\n",
              "      <td>0.822252</td>\n",
              "      <td>0.006688</td>\n",
              "      <td>0.001886</td>\n",
              "      <td>0.676099</td>\n",
              "      <td>0.998219</td>\n",
              "      <td>DERMASON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13608</th>\n",
              "      <td>42139</td>\n",
              "      <td>759.321</td>\n",
              "      <td>281.539928</td>\n",
              "      <td>191.187979</td>\n",
              "      <td>1.472582</td>\n",
              "      <td>0.734065</td>\n",
              "      <td>42569</td>\n",
              "      <td>231.631261</td>\n",
              "      <td>0.729932</td>\n",
              "      <td>0.989899</td>\n",
              "      <td>0.918424</td>\n",
              "      <td>0.822730</td>\n",
              "      <td>0.006681</td>\n",
              "      <td>0.001888</td>\n",
              "      <td>0.676884</td>\n",
              "      <td>0.996767</td>\n",
              "      <td>DERMASON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13609</th>\n",
              "      <td>42147</td>\n",
              "      <td>763.779</td>\n",
              "      <td>283.382636</td>\n",
              "      <td>190.275731</td>\n",
              "      <td>1.489326</td>\n",
              "      <td>0.741055</td>\n",
              "      <td>42667</td>\n",
              "      <td>231.653248</td>\n",
              "      <td>0.705389</td>\n",
              "      <td>0.987813</td>\n",
              "      <td>0.907906</td>\n",
              "      <td>0.817457</td>\n",
              "      <td>0.006724</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>0.668237</td>\n",
              "      <td>0.995222</td>\n",
              "      <td>DERMASON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13610</th>\n",
              "      <td>42159</td>\n",
              "      <td>772.237</td>\n",
              "      <td>295.142741</td>\n",
              "      <td>182.204716</td>\n",
              "      <td>1.619841</td>\n",
              "      <td>0.786693</td>\n",
              "      <td>42600</td>\n",
              "      <td>231.686223</td>\n",
              "      <td>0.788962</td>\n",
              "      <td>0.989648</td>\n",
              "      <td>0.888380</td>\n",
              "      <td>0.784997</td>\n",
              "      <td>0.007001</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.616221</td>\n",
              "      <td>0.998180</td>\n",
              "      <td>DERMASON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13611 rows Ã— 17 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c0717b9-db48-4740-8450-e1075e505e9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c0717b9-db48-4740-8450-e1075e505e9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c0717b9-db48-4740-8450-e1075e505e9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d7c15748-77f5-4184-b8b7-98794f45762d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d7c15748-77f5-4184-b8b7-98794f45762d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d7c15748-77f5-4184-b8b7-98794f45762d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building a Neural Network"
      ],
      "metadata": {
        "id": "zKxE2W1hZKeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pV6SYT1oYpIV"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input , hidden , num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(input , hidden )\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        out1 = self.l1(x)\n",
        "        out2 = self.relu(out1)\n",
        "        outf = self.l2(out2)\n",
        "\n",
        "        return outf\n",
        "\n",
        "model = MLP(input, hidden, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.parameters())\n",
        "\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "TywTK_wtQo38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loss and Optimizer\n"
      ],
      "metadata": {
        "id": "nko41tsalbvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Loss = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters() , lr= 0.001)"
      ],
      "metadata": {
        "id": "JkhvV3FAlgbj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset and Training, vallidation, test split"
      ],
      "metadata": {
        "id": "QlzvcRVXeXMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n"
      ],
      "metadata": {
        "id": "WCF8ocVQIbzD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class custom_dataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super(custom_dataset, self).__init__()\n",
        "        self.df = pd.read_excel(\"/content/Dry_Bean_Dataset.xlsx\")\n",
        "        self.scaled_data = scaler.fit_transform(self.df.iloc[:, 0:16].values)\n",
        "        self.labels = label_encoder.fit_transform(df['Class'].values)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = torch.tensor(self.scaled_data[idx] , dtype= torch.float32)\n",
        "        Y = torch.tensor(self.labels[idx], dtype = torch.long)\n",
        "\n",
        "        return X , Y"
      ],
      "metadata": {
        "id": "qZ9YOT190lKa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = custom_dataset()"
      ],
      "metadata": {
        "id": "ItO7E2is50G5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI8A71LZ6Ofv",
        "outputId": "76128e66-ff85-498b-d2c6-0ec0727bfacc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13611"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set, val_set = torch.utils.data.random_split(dataset, [0.6 , 0.2 , 0.2])"
      ],
      "metadata": {
        "id": "vdzmk63C511G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,batch_size=batch_size, shuffle=True\n",
        "   )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "      test_set, batch_size=batch_size, shuffle=False\n",
        "     )\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "      val_set, batch_size=batch_size, shuffle=False\n",
        "     )"
      ],
      "metadata": {
        "id": "U0FAC-y_BGsC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "QANAYyl4lgr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    loss = 0\n",
        "    for i, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        # reset the gradients back to zero\n",
        "        # PyTorch accumulates gradients on subsequent backward passes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #forward pass\n",
        "        outputs = model(features)\n",
        "\n",
        "        # compute loss\n",
        "        train_loss = Loss(outputs, labels)\n",
        "        print(train_loss,\"lol\")\n",
        "\n",
        "        # compute accumulated gradients\n",
        "        train_loss.backward()\n",
        "\n",
        "        # parameter update based on current gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # add the mini-batch training loss to epoch loss\n",
        "        loss += train_loss.item()\n",
        "        print(loss,\" huu\")\n",
        "\n",
        "    # compute the epoch training loss\n",
        "    loss = loss / len(train_loader)\n",
        "\n",
        "    # display the epoch training loss\n",
        "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, 10, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXqeJPKWjDtz",
        "outputId": "17f14744-377f-4d64-d61d-1b9dfaabd21a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8852, grad_fn=<NllLossBackward0>) lol\n",
            "1.885239601135254  huu\n",
            "tensor(1.8167, grad_fn=<NllLossBackward0>) lol\n",
            "3.7019662857055664  huu\n",
            "tensor(1.8693, grad_fn=<NllLossBackward0>) lol\n",
            "5.571231245994568  huu\n",
            "tensor(1.8619, grad_fn=<NllLossBackward0>) lol\n",
            "7.4331135749816895  huu\n",
            "tensor(1.7985, grad_fn=<NllLossBackward0>) lol\n",
            "9.231640458106995  huu\n",
            "tensor(1.8359, grad_fn=<NllLossBackward0>) lol\n",
            "11.067496061325073  huu\n",
            "tensor(1.7470, grad_fn=<NllLossBackward0>) lol\n",
            "12.814459562301636  huu\n",
            "tensor(1.7851, grad_fn=<NllLossBackward0>) lol\n",
            "14.599597454071045  huu\n",
            "tensor(1.7092, grad_fn=<NllLossBackward0>) lol\n",
            "16.30878436565399  huu\n",
            "tensor(1.6878, grad_fn=<NllLossBackward0>) lol\n",
            "17.996574759483337  huu\n",
            "tensor(1.6589, grad_fn=<NllLossBackward0>) lol\n",
            "19.65548086166382  huu\n",
            "tensor(1.6029, grad_fn=<NllLossBackward0>) lol\n",
            "21.258384704589844  huu\n",
            "tensor(1.6809, grad_fn=<NllLossBackward0>) lol\n",
            "22.939246654510498  huu\n",
            "tensor(1.5621, grad_fn=<NllLossBackward0>) lol\n",
            "24.501323223114014  huu\n",
            "tensor(1.5361, grad_fn=<NllLossBackward0>) lol\n",
            "26.037466049194336  huu\n",
            "tensor(1.5515, grad_fn=<NllLossBackward0>) lol\n",
            "27.588935375213623  huu\n",
            "tensor(1.4639, grad_fn=<NllLossBackward0>) lol\n",
            "29.05279815196991  huu\n",
            "tensor(1.5075, grad_fn=<NllLossBackward0>) lol\n",
            "30.560327887535095  huu\n",
            "tensor(1.5093, grad_fn=<NllLossBackward0>) lol\n",
            "32.06958520412445  huu\n",
            "tensor(1.4865, grad_fn=<NllLossBackward0>) lol\n",
            "33.556102871894836  huu\n",
            "tensor(1.4573, grad_fn=<NllLossBackward0>) lol\n",
            "35.01338303089142  huu\n",
            "tensor(1.4247, grad_fn=<NllLossBackward0>) lol\n",
            "36.43812274932861  huu\n",
            "tensor(1.4091, grad_fn=<NllLossBackward0>) lol\n",
            "37.84726917743683  huu\n",
            "tensor(1.3569, grad_fn=<NllLossBackward0>) lol\n",
            "39.204147815704346  huu\n",
            "tensor(1.3163, grad_fn=<NllLossBackward0>) lol\n",
            "40.52041947841644  huu\n",
            "tensor(1.3207, grad_fn=<NllLossBackward0>) lol\n",
            "41.841092586517334  huu\n",
            "tensor(1.3144, grad_fn=<NllLossBackward0>) lol\n",
            "43.155490040779114  huu\n",
            "tensor(1.4348, grad_fn=<NllLossBackward0>) lol\n",
            "44.59033787250519  huu\n",
            "tensor(1.3473, grad_fn=<NllLossBackward0>) lol\n",
            "45.937593936920166  huu\n",
            "tensor(1.3314, grad_fn=<NllLossBackward0>) lol\n",
            "47.269028067588806  huu\n",
            "tensor(1.2131, grad_fn=<NllLossBackward0>) lol\n",
            "48.48217523097992  huu\n",
            "tensor(1.1705, grad_fn=<NllLossBackward0>) lol\n",
            "49.65268695354462  huu\n",
            "tensor(1.2602, grad_fn=<NllLossBackward0>) lol\n",
            "50.91285765171051  huu\n",
            "tensor(1.2286, grad_fn=<NllLossBackward0>) lol\n",
            "52.14145648479462  huu\n",
            "tensor(1.2235, grad_fn=<NllLossBackward0>) lol\n",
            "53.36491870880127  huu\n",
            "tensor(1.1910, grad_fn=<NllLossBackward0>) lol\n",
            "54.55592882633209  huu\n",
            "tensor(1.1837, grad_fn=<NllLossBackward0>) lol\n",
            "55.73965299129486  huu\n",
            "tensor(1.1543, grad_fn=<NllLossBackward0>) lol\n",
            "56.893961787223816  huu\n",
            "tensor(1.1270, grad_fn=<NllLossBackward0>) lol\n",
            "58.020986914634705  huu\n",
            "tensor(1.0943, grad_fn=<NllLossBackward0>) lol\n",
            "59.115267634391785  huu\n",
            "tensor(1.0858, grad_fn=<NllLossBackward0>) lol\n",
            "60.201035380363464  huu\n",
            "tensor(1.0808, grad_fn=<NllLossBackward0>) lol\n",
            "61.281858801841736  huu\n",
            "tensor(1.0764, grad_fn=<NllLossBackward0>) lol\n",
            "62.35823655128479  huu\n",
            "tensor(1.0642, grad_fn=<NllLossBackward0>) lol\n",
            "63.42244207859039  huu\n",
            "tensor(1.0057, grad_fn=<NllLossBackward0>) lol\n",
            "64.42814898490906  huu\n",
            "tensor(1.1092, grad_fn=<NllLossBackward0>) lol\n",
            "65.53735744953156  huu\n",
            "tensor(1.0529, grad_fn=<NllLossBackward0>) lol\n",
            "66.590301156044  huu\n",
            "tensor(0.9863, grad_fn=<NllLossBackward0>) lol\n",
            "67.57658207416534  huu\n",
            "tensor(0.9335, grad_fn=<NllLossBackward0>) lol\n",
            "68.51006591320038  huu\n",
            "tensor(0.9733, grad_fn=<NllLossBackward0>) lol\n",
            "69.4833356142044  huu\n",
            "tensor(0.9237, grad_fn=<NllLossBackward0>) lol\n",
            "70.40698766708374  huu\n",
            "tensor(0.9685, grad_fn=<NllLossBackward0>) lol\n",
            "71.37544697523117  huu\n",
            "tensor(0.9567, grad_fn=<NllLossBackward0>) lol\n",
            "72.33217614889145  huu\n",
            "tensor(0.9465, grad_fn=<NllLossBackward0>) lol\n",
            "73.27864760160446  huu\n",
            "tensor(0.8835, grad_fn=<NllLossBackward0>) lol\n",
            "74.16211730241776  huu\n",
            "tensor(0.9369, grad_fn=<NllLossBackward0>) lol\n",
            "75.09899473190308  huu\n",
            "tensor(1.0625, grad_fn=<NllLossBackward0>) lol\n",
            "76.16149485111237  huu\n",
            "tensor(0.8361, grad_fn=<NllLossBackward0>) lol\n",
            "76.99754625558853  huu\n",
            "tensor(0.9126, grad_fn=<NllLossBackward0>) lol\n",
            "77.91012889146805  huu\n",
            "tensor(0.8499, grad_fn=<NllLossBackward0>) lol\n",
            "78.7600411772728  huu\n",
            "tensor(0.9184, grad_fn=<NllLossBackward0>) lol\n",
            "79.6784496307373  huu\n",
            "tensor(0.8581, grad_fn=<NllLossBackward0>) lol\n",
            "80.53650665283203  huu\n",
            "tensor(0.8234, grad_fn=<NllLossBackward0>) lol\n",
            "81.35987490415573  huu\n",
            "tensor(0.9062, grad_fn=<NllLossBackward0>) lol\n",
            "82.2661075592041  huu\n",
            "tensor(0.8522, grad_fn=<NllLossBackward0>) lol\n",
            "83.11832290887833  huu\n",
            "tensor(0.8400, grad_fn=<NllLossBackward0>) lol\n",
            "83.95832985639572  huu\n",
            "tensor(0.8144, grad_fn=<NllLossBackward0>) lol\n",
            "84.77275323867798  huu\n",
            "tensor(0.7429, grad_fn=<NllLossBackward0>) lol\n",
            "85.5156900882721  huu\n",
            "tensor(0.7733, grad_fn=<NllLossBackward0>) lol\n",
            "86.28894197940826  huu\n",
            "tensor(0.8275, grad_fn=<NllLossBackward0>) lol\n",
            "87.11640483140945  huu\n",
            "tensor(0.7511, grad_fn=<NllLossBackward0>) lol\n",
            "87.86745846271515  huu\n",
            "tensor(0.7419, grad_fn=<NllLossBackward0>) lol\n",
            "88.60936087369919  huu\n",
            "tensor(0.6482, grad_fn=<NllLossBackward0>) lol\n",
            "89.25752770900726  huu\n",
            "tensor(0.7424, grad_fn=<NllLossBackward0>) lol\n",
            "89.99995142221451  huu\n",
            "tensor(0.6838, grad_fn=<NllLossBackward0>) lol\n",
            "90.68373513221741  huu\n",
            "tensor(0.7328, grad_fn=<NllLossBackward0>) lol\n",
            "91.41654425859451  huu\n",
            "tensor(0.8131, grad_fn=<NllLossBackward0>) lol\n",
            "92.22959578037262  huu\n",
            "tensor(0.7914, grad_fn=<NllLossBackward0>) lol\n",
            "93.02100437879562  huu\n",
            "tensor(0.7177, grad_fn=<NllLossBackward0>) lol\n",
            "93.73874914646149  huu\n",
            "tensor(0.7264, grad_fn=<NllLossBackward0>) lol\n",
            "94.46513456106186  huu\n",
            "tensor(0.7416, grad_fn=<NllLossBackward0>) lol\n",
            "95.20675963163376  huu\n",
            "tensor(0.8438, grad_fn=<NllLossBackward0>) lol\n",
            "96.05058085918427  huu\n",
            "epoch : 1/10, recon loss = 1.17134855\n",
            "tensor(0.7418, grad_fn=<NllLossBackward0>) lol\n",
            "0.7417999505996704  huu\n",
            "tensor(0.6500, grad_fn=<NllLossBackward0>) lol\n",
            "1.3918415904045105  huu\n",
            "tensor(0.6123, grad_fn=<NllLossBackward0>) lol\n",
            "2.0041500329971313  huu\n",
            "tensor(0.6793, grad_fn=<NllLossBackward0>) lol\n",
            "2.683471143245697  huu\n",
            "tensor(0.6623, grad_fn=<NllLossBackward0>) lol\n",
            "3.345745623111725  huu\n",
            "tensor(0.5683, grad_fn=<NllLossBackward0>) lol\n",
            "3.9140272736549377  huu\n",
            "tensor(0.6074, grad_fn=<NllLossBackward0>) lol\n",
            "4.521384119987488  huu\n",
            "tensor(0.6532, grad_fn=<NllLossBackward0>) lol\n",
            "5.174550473690033  huu\n",
            "tensor(0.6155, grad_fn=<NllLossBackward0>) lol\n",
            "5.790012001991272  huu\n",
            "tensor(0.6152, grad_fn=<NllLossBackward0>) lol\n",
            "6.405233383178711  huu\n",
            "tensor(0.6232, grad_fn=<NllLossBackward0>) lol\n",
            "7.028424084186554  huu\n",
            "tensor(0.6667, grad_fn=<NllLossBackward0>) lol\n",
            "7.695138514041901  huu\n",
            "tensor(0.6341, grad_fn=<NllLossBackward0>) lol\n",
            "8.329209387302399  huu\n",
            "tensor(0.6165, grad_fn=<NllLossBackward0>) lol\n",
            "8.945699691772461  huu\n",
            "tensor(0.6060, grad_fn=<NllLossBackward0>) lol\n",
            "9.551676750183105  huu\n",
            "tensor(0.7000, grad_fn=<NllLossBackward0>) lol\n",
            "10.251682102680206  huu\n",
            "tensor(0.6036, grad_fn=<NllLossBackward0>) lol\n",
            "10.855251848697662  huu\n",
            "tensor(0.6075, grad_fn=<NllLossBackward0>) lol\n",
            "11.462801277637482  huu\n",
            "tensor(0.5912, grad_fn=<NllLossBackward0>) lol\n",
            "12.053980648517609  huu\n",
            "tensor(0.5898, grad_fn=<NllLossBackward0>) lol\n",
            "12.643796801567078  huu\n",
            "tensor(0.6186, grad_fn=<NllLossBackward0>) lol\n",
            "13.262361645698547  huu\n",
            "tensor(0.5425, grad_fn=<NllLossBackward0>) lol\n",
            "13.804815590381622  huu\n",
            "tensor(0.6228, grad_fn=<NllLossBackward0>) lol\n",
            "14.427649736404419  huu\n",
            "tensor(0.5772, grad_fn=<NllLossBackward0>) lol\n",
            "15.00480717420578  huu\n",
            "tensor(0.6197, grad_fn=<NllLossBackward0>) lol\n",
            "15.624474465847015  huu\n",
            "tensor(0.6199, grad_fn=<NllLossBackward0>) lol\n",
            "16.24440175294876  huu\n",
            "tensor(0.5022, grad_fn=<NllLossBackward0>) lol\n",
            "16.746556997299194  huu\n",
            "tensor(0.6351, grad_fn=<NllLossBackward0>) lol\n",
            "17.381609439849854  huu\n",
            "tensor(0.5585, grad_fn=<NllLossBackward0>) lol\n",
            "17.940148770809174  huu\n",
            "tensor(0.5717, grad_fn=<NllLossBackward0>) lol\n",
            "18.511812686920166  huu\n",
            "tensor(0.5668, grad_fn=<NllLossBackward0>) lol\n",
            "19.0786030292511  huu\n",
            "tensor(0.5203, grad_fn=<NllLossBackward0>) lol\n",
            "19.598942518234253  huu\n",
            "tensor(0.5203, grad_fn=<NllLossBackward0>) lol\n",
            "20.119241416454315  huu\n",
            "tensor(0.5481, grad_fn=<NllLossBackward0>) lol\n",
            "20.667347490787506  huu\n",
            "tensor(0.5242, grad_fn=<NllLossBackward0>) lol\n",
            "21.19159173965454  huu\n",
            "tensor(0.5076, grad_fn=<NllLossBackward0>) lol\n",
            "21.69919753074646  huu\n",
            "tensor(0.5487, grad_fn=<NllLossBackward0>) lol\n",
            "22.247901380062103  huu\n",
            "tensor(0.5014, grad_fn=<NllLossBackward0>) lol\n",
            "22.74930828809738  huu\n",
            "tensor(0.4511, grad_fn=<NllLossBackward0>) lol\n",
            "23.200362354516983  huu\n",
            "tensor(0.4520, grad_fn=<NllLossBackward0>) lol\n",
            "23.65231555700302  huu\n",
            "tensor(0.4592, grad_fn=<NllLossBackward0>) lol\n",
            "24.111550599336624  huu\n",
            "tensor(0.4644, grad_fn=<NllLossBackward0>) lol\n",
            "24.575938999652863  huu\n",
            "tensor(0.4755, grad_fn=<NllLossBackward0>) lol\n",
            "25.051445245742798  huu\n",
            "tensor(0.5305, grad_fn=<NllLossBackward0>) lol\n",
            "25.581980228424072  huu\n",
            "tensor(0.4997, grad_fn=<NllLossBackward0>) lol\n",
            "26.08170771598816  huu\n",
            "tensor(0.5438, grad_fn=<NllLossBackward0>) lol\n",
            "26.62550324201584  huu\n",
            "tensor(0.4733, grad_fn=<NllLossBackward0>) lol\n",
            "27.098759204149246  huu\n",
            "tensor(0.4973, grad_fn=<NllLossBackward0>) lol\n",
            "27.596026688814163  huu\n",
            "tensor(0.4796, grad_fn=<NllLossBackward0>) lol\n",
            "28.07563304901123  huu\n",
            "tensor(0.4308, grad_fn=<NllLossBackward0>) lol\n",
            "28.50640121102333  huu\n",
            "tensor(0.4210, grad_fn=<NllLossBackward0>) lol\n",
            "28.927422165870667  huu\n",
            "tensor(0.4572, grad_fn=<NllLossBackward0>) lol\n",
            "29.38459324836731  huu\n",
            "tensor(0.5819, grad_fn=<NllLossBackward0>) lol\n",
            "29.966465711593628  huu\n",
            "tensor(0.5735, grad_fn=<NllLossBackward0>) lol\n",
            "30.539970576763153  huu\n",
            "tensor(0.4359, grad_fn=<NllLossBackward0>) lol\n",
            "30.9758280813694  huu\n",
            "tensor(0.5054, grad_fn=<NllLossBackward0>) lol\n",
            "31.481213301420212  huu\n",
            "tensor(0.4455, grad_fn=<NllLossBackward0>) lol\n",
            "31.926705062389374  huu\n",
            "tensor(0.4207, grad_fn=<NllLossBackward0>) lol\n",
            "32.34735995531082  huu\n",
            "tensor(0.4672, grad_fn=<NllLossBackward0>) lol\n",
            "32.81460493803024  huu\n",
            "tensor(0.3443, grad_fn=<NllLossBackward0>) lol\n",
            "33.158894419670105  huu\n",
            "tensor(0.4156, grad_fn=<NllLossBackward0>) lol\n",
            "33.57445013523102  huu\n",
            "tensor(0.4267, grad_fn=<NllLossBackward0>) lol\n",
            "34.00119215250015  huu\n",
            "tensor(0.4318, grad_fn=<NllLossBackward0>) lol\n",
            "34.43302622437477  huu\n",
            "tensor(0.3746, grad_fn=<NllLossBackward0>) lol\n",
            "34.80765891075134  huu\n",
            "tensor(0.4105, grad_fn=<NllLossBackward0>) lol\n",
            "35.21814599633217  huu\n",
            "tensor(0.3850, grad_fn=<NllLossBackward0>) lol\n",
            "35.603116273880005  huu\n",
            "tensor(0.3960, grad_fn=<NllLossBackward0>) lol\n",
            "35.9990930557251  huu\n",
            "tensor(0.4061, grad_fn=<NllLossBackward0>) lol\n",
            "36.40524196624756  huu\n",
            "tensor(0.3881, grad_fn=<NllLossBackward0>) lol\n",
            "36.79331251978874  huu\n",
            "tensor(0.3983, grad_fn=<NllLossBackward0>) lol\n",
            "37.19159433245659  huu\n",
            "tensor(0.4850, grad_fn=<NllLossBackward0>) lol\n",
            "37.67659565806389  huu\n",
            "tensor(0.3535, grad_fn=<NllLossBackward0>) lol\n",
            "38.03013610839844  huu\n",
            "tensor(0.4472, grad_fn=<NllLossBackward0>) lol\n",
            "38.47733399271965  huu\n",
            "tensor(0.3781, grad_fn=<NllLossBackward0>) lol\n",
            "38.85542565584183  huu\n",
            "tensor(0.4075, grad_fn=<NllLossBackward0>) lol\n",
            "39.26288589835167  huu\n",
            "tensor(0.3715, grad_fn=<NllLossBackward0>) lol\n",
            "39.63443273305893  huu\n",
            "tensor(0.3800, grad_fn=<NllLossBackward0>) lol\n",
            "40.014386147260666  huu\n",
            "tensor(0.3429, grad_fn=<NllLossBackward0>) lol\n",
            "40.35732063651085  huu\n",
            "tensor(0.3980, grad_fn=<NllLossBackward0>) lol\n",
            "40.75528660416603  huu\n",
            "tensor(0.3699, grad_fn=<NllLossBackward0>) lol\n",
            "41.12515479326248  huu\n",
            "tensor(0.4735, grad_fn=<NllLossBackward0>) lol\n",
            "41.598627269268036  huu\n",
            "tensor(0.4302, grad_fn=<NllLossBackward0>) lol\n",
            "42.028806269168854  huu\n",
            "epoch : 2/10, recon loss = 0.51254642\n",
            "tensor(0.5518, grad_fn=<NllLossBackward0>) lol\n",
            "0.5518167614936829  huu\n",
            "tensor(0.3898, grad_fn=<NllLossBackward0>) lol\n",
            "0.941625326871872  huu\n",
            "tensor(0.2926, grad_fn=<NllLossBackward0>) lol\n",
            "1.2342442274093628  huu\n",
            "tensor(0.3652, grad_fn=<NllLossBackward0>) lol\n",
            "1.5994048118591309  huu\n",
            "tensor(0.4554, grad_fn=<NllLossBackward0>) lol\n",
            "2.0548036098480225  huu\n",
            "tensor(0.3413, grad_fn=<NllLossBackward0>) lol\n",
            "2.396062880754471  huu\n",
            "tensor(0.4001, grad_fn=<NllLossBackward0>) lol\n",
            "2.7961250245571136  huu\n",
            "tensor(0.4127, grad_fn=<NllLossBackward0>) lol\n",
            "3.2088442146778107  huu\n",
            "tensor(0.4606, grad_fn=<NllLossBackward0>) lol\n",
            "3.6694069504737854  huu\n",
            "tensor(0.3777, grad_fn=<NllLossBackward0>) lol\n",
            "4.0470713675022125  huu\n",
            "tensor(0.3829, grad_fn=<NllLossBackward0>) lol\n",
            "4.429983556270599  huu\n",
            "tensor(0.3658, grad_fn=<NllLossBackward0>) lol\n",
            "4.795800894498825  huu\n",
            "tensor(0.3762, grad_fn=<NllLossBackward0>) lol\n",
            "5.172025561332703  huu\n",
            "tensor(0.4299, grad_fn=<NllLossBackward0>) lol\n",
            "5.601948589086533  huu\n",
            "tensor(0.4103, grad_fn=<NllLossBackward0>) lol\n",
            "6.012270629405975  huu\n",
            "tensor(0.3816, grad_fn=<NllLossBackward0>) lol\n",
            "6.3938489854335785  huu\n",
            "tensor(0.3137, grad_fn=<NllLossBackward0>) lol\n",
            "6.7075150310993195  huu\n",
            "tensor(0.3416, grad_fn=<NllLossBackward0>) lol\n",
            "7.049067050218582  huu\n",
            "tensor(0.3752, grad_fn=<NllLossBackward0>) lol\n",
            "7.424272865056992  huu\n",
            "tensor(0.2938, grad_fn=<NllLossBackward0>) lol\n",
            "7.718085020780563  huu\n",
            "tensor(0.3692, grad_fn=<NllLossBackward0>) lol\n",
            "8.087261319160461  huu\n",
            "tensor(0.3374, grad_fn=<NllLossBackward0>) lol\n",
            "8.424687683582306  huu\n",
            "tensor(0.3817, grad_fn=<NllLossBackward0>) lol\n",
            "8.806358128786087  huu\n",
            "tensor(0.3259, grad_fn=<NllLossBackward0>) lol\n",
            "9.132227778434753  huu\n",
            "tensor(0.3191, grad_fn=<NllLossBackward0>) lol\n",
            "9.451314568519592  huu\n",
            "tensor(0.3047, grad_fn=<NllLossBackward0>) lol\n",
            "9.756038755178452  huu\n",
            "tensor(0.2911, grad_fn=<NllLossBackward0>) lol\n",
            "10.047099590301514  huu\n",
            "tensor(0.3402, grad_fn=<NllLossBackward0>) lol\n",
            "10.387256562709808  huu\n",
            "tensor(0.3964, grad_fn=<NllLossBackward0>) lol\n",
            "10.783632457256317  huu\n",
            "tensor(0.3209, grad_fn=<NllLossBackward0>) lol\n",
            "11.10458105802536  huu\n",
            "tensor(0.3045, grad_fn=<NllLossBackward0>) lol\n",
            "11.409080117940903  huu\n",
            "tensor(0.4049, grad_fn=<NllLossBackward0>) lol\n",
            "11.813998371362686  huu\n",
            "tensor(0.3637, grad_fn=<NllLossBackward0>) lol\n",
            "12.177698284387589  huu\n",
            "tensor(0.3473, grad_fn=<NllLossBackward0>) lol\n",
            "12.524961858987808  huu\n",
            "tensor(0.3406, grad_fn=<NllLossBackward0>) lol\n",
            "12.865515232086182  huu\n",
            "tensor(0.3399, grad_fn=<NllLossBackward0>) lol\n",
            "13.205388098955154  huu\n",
            "tensor(0.3587, grad_fn=<NllLossBackward0>) lol\n",
            "13.564049750566483  huu\n",
            "tensor(0.3774, grad_fn=<NllLossBackward0>) lol\n",
            "13.94148063659668  huu\n",
            "tensor(0.3022, grad_fn=<NllLossBackward0>) lol\n",
            "14.243636816740036  huu\n",
            "tensor(0.2862, grad_fn=<NllLossBackward0>) lol\n",
            "14.529816210269928  huu\n",
            "tensor(0.2847, grad_fn=<NllLossBackward0>) lol\n",
            "14.814544647932053  huu\n",
            "tensor(0.3479, grad_fn=<NllLossBackward0>) lol\n",
            "15.162453055381775  huu\n",
            "tensor(0.3254, grad_fn=<NllLossBackward0>) lol\n",
            "15.487832516431808  huu\n",
            "tensor(0.2323, grad_fn=<NllLossBackward0>) lol\n",
            "15.720165610313416  huu\n",
            "tensor(0.3196, grad_fn=<NllLossBackward0>) lol\n",
            "16.039723843336105  huu\n",
            "tensor(0.3325, grad_fn=<NllLossBackward0>) lol\n",
            "16.37224742770195  huu\n",
            "tensor(0.3246, grad_fn=<NllLossBackward0>) lol\n",
            "16.696876138448715  huu\n",
            "tensor(0.3335, grad_fn=<NllLossBackward0>) lol\n",
            "17.03033870458603  huu\n",
            "tensor(0.3704, grad_fn=<NllLossBackward0>) lol\n",
            "17.40077430009842  huu\n",
            "tensor(0.1744, grad_fn=<NllLossBackward0>) lol\n",
            "17.57513339817524  huu\n",
            "tensor(0.3738, grad_fn=<NllLossBackward0>) lol\n",
            "17.948930844664574  huu\n",
            "tensor(0.3129, grad_fn=<NllLossBackward0>) lol\n",
            "18.26182360947132  huu\n",
            "tensor(0.3277, grad_fn=<NllLossBackward0>) lol\n",
            "18.589560464024544  huu\n",
            "tensor(0.3332, grad_fn=<NllLossBackward0>) lol\n",
            "18.922770634293556  huu\n",
            "tensor(0.3565, grad_fn=<NllLossBackward0>) lol\n",
            "19.279257521033287  huu\n",
            "tensor(0.3926, grad_fn=<NllLossBackward0>) lol\n",
            "19.671827659010887  huu\n",
            "tensor(0.2679, grad_fn=<NllLossBackward0>) lol\n",
            "19.93967865407467  huu\n",
            "tensor(0.3827, grad_fn=<NllLossBackward0>) lol\n",
            "20.322399005293846  huu\n",
            "tensor(0.2631, grad_fn=<NllLossBackward0>) lol\n",
            "20.585529163479805  huu\n",
            "tensor(0.2926, grad_fn=<NllLossBackward0>) lol\n",
            "20.878129437565804  huu\n",
            "tensor(0.2474, grad_fn=<NllLossBackward0>) lol\n",
            "21.12554667890072  huu\n",
            "tensor(0.2517, grad_fn=<NllLossBackward0>) lol\n",
            "21.377204850316048  huu\n",
            "tensor(0.4070, grad_fn=<NllLossBackward0>) lol\n",
            "21.7841567248106  huu\n",
            "tensor(0.3378, grad_fn=<NllLossBackward0>) lol\n",
            "22.121959075331688  huu\n",
            "tensor(0.3472, grad_fn=<NllLossBackward0>) lol\n",
            "22.469109281897545  huu\n",
            "tensor(0.2309, grad_fn=<NllLossBackward0>) lol\n",
            "22.70003207027912  huu\n",
            "tensor(0.4134, grad_fn=<NllLossBackward0>) lol\n",
            "23.113472536206245  huu\n",
            "tensor(0.2021, grad_fn=<NllLossBackward0>) lol\n",
            "23.31562167406082  huu\n",
            "tensor(0.3994, grad_fn=<NllLossBackward0>) lol\n",
            "23.715018451213837  huu\n",
            "tensor(0.3209, grad_fn=<NllLossBackward0>) lol\n",
            "24.035944670438766  huu\n",
            "tensor(0.2832, grad_fn=<NllLossBackward0>) lol\n",
            "24.319134175777435  huu\n",
            "tensor(0.3176, grad_fn=<NllLossBackward0>) lol\n",
            "24.636709451675415  huu\n",
            "tensor(0.2470, grad_fn=<NllLossBackward0>) lol\n",
            "24.88366962969303  huu\n",
            "tensor(0.2363, grad_fn=<NllLossBackward0>) lol\n",
            "25.119967982172966  huu\n",
            "tensor(0.3522, grad_fn=<NllLossBackward0>) lol\n",
            "25.472121730446815  huu\n",
            "tensor(0.3279, grad_fn=<NllLossBackward0>) lol\n",
            "25.80002410709858  huu\n",
            "tensor(0.3366, grad_fn=<NllLossBackward0>) lol\n",
            "26.136647328734398  huu\n",
            "tensor(0.2901, grad_fn=<NllLossBackward0>) lol\n",
            "26.426745876669884  huu\n",
            "tensor(0.3429, grad_fn=<NllLossBackward0>) lol\n",
            "26.769675567746162  huu\n",
            "tensor(0.2958, grad_fn=<NllLossBackward0>) lol\n",
            "27.065499857068062  huu\n",
            "tensor(0.2058, grad_fn=<NllLossBackward0>) lol\n",
            "27.271281465888023  huu\n",
            "tensor(0.3880, grad_fn=<NllLossBackward0>) lol\n",
            "27.659262880682945  huu\n",
            "epoch : 3/10, recon loss = 0.33730808\n",
            "tensor(0.3572, grad_fn=<NllLossBackward0>) lol\n",
            "0.3572447597980499  huu\n",
            "tensor(0.2657, grad_fn=<NllLossBackward0>) lol\n",
            "0.6229515373706818  huu\n",
            "tensor(0.3396, grad_fn=<NllLossBackward0>) lol\n",
            "0.9625372588634491  huu\n",
            "tensor(0.2914, grad_fn=<NllLossBackward0>) lol\n",
            "1.2539308667182922  huu\n",
            "tensor(0.1993, grad_fn=<NllLossBackward0>) lol\n",
            "1.4532523453235626  huu\n",
            "tensor(0.3317, grad_fn=<NllLossBackward0>) lol\n",
            "1.784950464963913  huu\n",
            "tensor(0.2937, grad_fn=<NllLossBackward0>) lol\n",
            "2.0786737501621246  huu\n",
            "tensor(0.2511, grad_fn=<NllLossBackward0>) lol\n",
            "2.329800456762314  huu\n",
            "tensor(0.2617, grad_fn=<NllLossBackward0>) lol\n",
            "2.5914600491523743  huu\n",
            "tensor(0.2819, grad_fn=<NllLossBackward0>) lol\n",
            "2.87331560254097  huu\n",
            "tensor(0.2788, grad_fn=<NllLossBackward0>) lol\n",
            "3.152107298374176  huu\n",
            "tensor(0.2546, grad_fn=<NllLossBackward0>) lol\n",
            "3.406751900911331  huu\n",
            "tensor(0.3641, grad_fn=<NllLossBackward0>) lol\n",
            "3.770815074443817  huu\n",
            "tensor(0.2470, grad_fn=<NllLossBackward0>) lol\n",
            "4.017792358994484  huu\n",
            "tensor(0.3125, grad_fn=<NllLossBackward0>) lol\n",
            "4.330327823758125  huu\n",
            "tensor(0.2892, grad_fn=<NllLossBackward0>) lol\n",
            "4.61950309574604  huu\n",
            "tensor(0.3309, grad_fn=<NllLossBackward0>) lol\n",
            "4.950393840670586  huu\n",
            "tensor(0.3153, grad_fn=<NllLossBackward0>) lol\n",
            "5.265711709856987  huu\n",
            "tensor(0.3137, grad_fn=<NllLossBackward0>) lol\n",
            "5.5794080048799515  huu\n",
            "tensor(0.2323, grad_fn=<NllLossBackward0>) lol\n",
            "5.811682656407356  huu\n",
            "tensor(0.3463, grad_fn=<NllLossBackward0>) lol\n",
            "6.157942995429039  huu\n",
            "tensor(0.3206, grad_fn=<NllLossBackward0>) lol\n",
            "6.478514537215233  huu\n",
            "tensor(0.3596, grad_fn=<NllLossBackward0>) lol\n",
            "6.838116809725761  huu\n",
            "tensor(0.1979, grad_fn=<NllLossBackward0>) lol\n",
            "7.03597067296505  huu\n",
            "tensor(0.2788, grad_fn=<NllLossBackward0>) lol\n",
            "7.3147933930158615  huu\n",
            "tensor(0.1997, grad_fn=<NllLossBackward0>) lol\n",
            "7.514498189091682  huu\n",
            "tensor(0.2786, grad_fn=<NllLossBackward0>) lol\n",
            "7.79314012825489  huu\n",
            "tensor(0.1930, grad_fn=<NllLossBackward0>) lol\n",
            "7.986182004213333  huu\n",
            "tensor(0.2481, grad_fn=<NllLossBackward0>) lol\n",
            "8.234299376606941  huu\n",
            "tensor(0.3199, grad_fn=<NllLossBackward0>) lol\n",
            "8.554223671555519  huu\n",
            "tensor(0.1893, grad_fn=<NllLossBackward0>) lol\n",
            "8.743545725941658  huu\n",
            "tensor(0.3981, grad_fn=<NllLossBackward0>) lol\n",
            "9.141630753874779  huu\n",
            "tensor(0.1951, grad_fn=<NllLossBackward0>) lol\n",
            "9.336728155612946  huu\n",
            "tensor(0.2252, grad_fn=<NllLossBackward0>) lol\n",
            "9.561907291412354  huu\n",
            "tensor(0.2050, grad_fn=<NllLossBackward0>) lol\n",
            "9.766921296715736  huu\n",
            "tensor(0.3303, grad_fn=<NllLossBackward0>) lol\n",
            "10.097222492098808  huu\n",
            "tensor(0.2818, grad_fn=<NllLossBackward0>) lol\n",
            "10.378994449973106  huu\n",
            "tensor(0.2104, grad_fn=<NllLossBackward0>) lol\n",
            "10.589427188038826  huu\n",
            "tensor(0.2024, grad_fn=<NllLossBackward0>) lol\n",
            "10.791805982589722  huu\n",
            "tensor(0.3487, grad_fn=<NllLossBackward0>) lol\n",
            "11.140508562326431  huu\n",
            "tensor(0.3335, grad_fn=<NllLossBackward0>) lol\n",
            "11.473970234394073  huu\n",
            "tensor(0.3304, grad_fn=<NllLossBackward0>) lol\n",
            "11.804386138916016  huu\n",
            "tensor(0.2368, grad_fn=<NllLossBackward0>) lol\n",
            "12.0411616563797  huu\n",
            "tensor(0.2161, grad_fn=<NllLossBackward0>) lol\n",
            "12.25727079808712  huu\n",
            "tensor(0.1727, grad_fn=<NllLossBackward0>) lol\n",
            "12.429956808686256  huu\n",
            "tensor(0.3173, grad_fn=<NllLossBackward0>) lol\n",
            "12.74723495543003  huu\n",
            "tensor(0.2535, grad_fn=<NllLossBackward0>) lol\n",
            "13.000768348574638  huu\n",
            "tensor(0.1895, grad_fn=<NllLossBackward0>) lol\n",
            "13.1903026252985  huu\n",
            "tensor(0.2443, grad_fn=<NllLossBackward0>) lol\n",
            "13.43457317352295  huu\n",
            "tensor(0.1796, grad_fn=<NllLossBackward0>) lol\n",
            "13.61420588195324  huu\n",
            "tensor(0.3765, grad_fn=<NllLossBackward0>) lol\n",
            "13.990661278367043  huu\n",
            "tensor(0.1873, grad_fn=<NllLossBackward0>) lol\n",
            "14.177956730127335  huu\n",
            "tensor(0.3668, grad_fn=<NllLossBackward0>) lol\n",
            "14.54475748538971  huu\n",
            "tensor(0.4134, grad_fn=<NllLossBackward0>) lol\n",
            "14.95811441540718  huu\n",
            "tensor(0.2573, grad_fn=<NllLossBackward0>) lol\n",
            "15.215374529361725  huu\n",
            "tensor(0.2584, grad_fn=<NllLossBackward0>) lol\n",
            "15.473811537027359  huu\n",
            "tensor(0.3036, grad_fn=<NllLossBackward0>) lol\n",
            "15.77738618850708  huu\n",
            "tensor(0.2640, grad_fn=<NllLossBackward0>) lol\n",
            "16.0414120554924  huu\n",
            "tensor(0.2476, grad_fn=<NllLossBackward0>) lol\n",
            "16.289032623171806  huu\n",
            "tensor(0.2612, grad_fn=<NllLossBackward0>) lol\n",
            "16.55019722878933  huu\n",
            "tensor(0.3020, grad_fn=<NllLossBackward0>) lol\n",
            "16.85216660797596  huu\n",
            "tensor(0.2004, grad_fn=<NllLossBackward0>) lol\n",
            "17.052575662732124  huu\n",
            "tensor(0.2690, grad_fn=<NllLossBackward0>) lol\n",
            "17.321587309241295  huu\n",
            "tensor(0.2605, grad_fn=<NllLossBackward0>) lol\n",
            "17.582108423113823  huu\n",
            "tensor(0.3412, grad_fn=<NllLossBackward0>) lol\n",
            "17.923323675990105  huu\n",
            "tensor(0.3264, grad_fn=<NllLossBackward0>) lol\n",
            "18.249742403626442  huu\n",
            "tensor(0.3454, grad_fn=<NllLossBackward0>) lol\n",
            "18.595096573233604  huu\n",
            "tensor(0.2502, grad_fn=<NllLossBackward0>) lol\n",
            "18.84529323875904  huu\n",
            "tensor(0.1996, grad_fn=<NllLossBackward0>) lol\n",
            "19.044859990477562  huu\n",
            "tensor(0.1651, grad_fn=<NllLossBackward0>) lol\n",
            "19.20991560816765  huu\n",
            "tensor(0.2633, grad_fn=<NllLossBackward0>) lol\n",
            "19.473238319158554  huu\n",
            "tensor(0.2125, grad_fn=<NllLossBackward0>) lol\n",
            "19.685720771551132  huu\n",
            "tensor(0.2762, grad_fn=<NllLossBackward0>) lol\n",
            "19.96187937259674  huu\n",
            "tensor(0.2859, grad_fn=<NllLossBackward0>) lol\n",
            "20.24776917695999  huu\n",
            "tensor(0.3118, grad_fn=<NllLossBackward0>) lol\n",
            "20.5596105158329  huu\n",
            "tensor(0.2412, grad_fn=<NllLossBackward0>) lol\n",
            "20.800807490944862  huu\n",
            "tensor(0.2228, grad_fn=<NllLossBackward0>) lol\n",
            "21.023576632142067  huu\n",
            "tensor(0.1993, grad_fn=<NllLossBackward0>) lol\n",
            "21.222866371273994  huu\n",
            "tensor(0.2613, grad_fn=<NllLossBackward0>) lol\n",
            "21.484208568930626  huu\n",
            "tensor(0.3847, grad_fn=<NllLossBackward0>) lol\n",
            "21.86893303692341  huu\n",
            "tensor(0.2636, grad_fn=<NllLossBackward0>) lol\n",
            "22.132506296038628  huu\n",
            "tensor(0.1957, grad_fn=<NllLossBackward0>) lol\n",
            "22.32817178964615  huu\n",
            "epoch : 4/10, recon loss = 0.27229478\n",
            "tensor(0.2852, grad_fn=<NllLossBackward0>) lol\n",
            "0.28517916798591614  huu\n",
            "tensor(0.2901, grad_fn=<NllLossBackward0>) lol\n",
            "0.5752888023853302  huu\n",
            "tensor(0.2432, grad_fn=<NllLossBackward0>) lol\n",
            "0.8185205012559891  huu\n",
            "tensor(0.2393, grad_fn=<NllLossBackward0>) lol\n",
            "1.057777851819992  huu\n",
            "tensor(0.2484, grad_fn=<NllLossBackward0>) lol\n",
            "1.306212455034256  huu\n",
            "tensor(0.2682, grad_fn=<NllLossBackward0>) lol\n",
            "1.5743756592273712  huu\n",
            "tensor(0.2223, grad_fn=<NllLossBackward0>) lol\n",
            "1.7967118173837662  huu\n",
            "tensor(0.2795, grad_fn=<NllLossBackward0>) lol\n",
            "2.076236918568611  huu\n",
            "tensor(0.2520, grad_fn=<NllLossBackward0>) lol\n",
            "2.328246220946312  huu\n",
            "tensor(0.3650, grad_fn=<NllLossBackward0>) lol\n",
            "2.6932766884565353  huu\n",
            "tensor(0.3410, grad_fn=<NllLossBackward0>) lol\n",
            "3.034230127930641  huu\n",
            "tensor(0.2556, grad_fn=<NllLossBackward0>) lol\n",
            "3.2898429483175278  huu\n",
            "tensor(0.3201, grad_fn=<NllLossBackward0>) lol\n",
            "3.609895095229149  huu\n",
            "tensor(0.2582, grad_fn=<NllLossBackward0>) lol\n",
            "3.8681229799985886  huu\n",
            "tensor(0.2361, grad_fn=<NllLossBackward0>) lol\n",
            "4.104179739952087  huu\n",
            "tensor(0.2266, grad_fn=<NllLossBackward0>) lol\n",
            "4.330808788537979  huu\n",
            "tensor(0.2625, grad_fn=<NllLossBackward0>) lol\n",
            "4.593312233686447  huu\n",
            "tensor(0.2323, grad_fn=<NllLossBackward0>) lol\n",
            "4.825567156076431  huu\n",
            "tensor(0.2254, grad_fn=<NllLossBackward0>) lol\n",
            "5.051011487841606  huu\n",
            "tensor(0.2374, grad_fn=<NllLossBackward0>) lol\n",
            "5.288412883877754  huu\n",
            "tensor(0.2373, grad_fn=<NllLossBackward0>) lol\n",
            "5.525751173496246  huu\n",
            "tensor(0.2137, grad_fn=<NllLossBackward0>) lol\n",
            "5.739490509033203  huu\n",
            "tensor(0.2656, grad_fn=<NllLossBackward0>) lol\n",
            "6.0050961673259735  huu\n",
            "tensor(0.2336, grad_fn=<NllLossBackward0>) lol\n",
            "6.2387132197618484  huu\n",
            "tensor(0.3735, grad_fn=<NllLossBackward0>) lol\n",
            "6.612232580780983  huu\n",
            "tensor(0.1311, grad_fn=<NllLossBackward0>) lol\n",
            "6.74338062107563  huu\n",
            "tensor(0.2528, grad_fn=<NllLossBackward0>) lol\n",
            "6.996173307299614  huu\n",
            "tensor(0.1811, grad_fn=<NllLossBackward0>) lol\n",
            "7.177297502756119  huu\n",
            "tensor(0.2072, grad_fn=<NllLossBackward0>) lol\n",
            "7.384469956159592  huu\n",
            "tensor(0.3361, grad_fn=<NllLossBackward0>) lol\n",
            "7.720567971467972  huu\n",
            "tensor(0.2344, grad_fn=<NllLossBackward0>) lol\n",
            "7.954958155751228  huu\n",
            "tensor(0.2269, grad_fn=<NllLossBackward0>) lol\n",
            "8.181865870952606  huu\n",
            "tensor(0.1577, grad_fn=<NllLossBackward0>) lol\n",
            "8.339550480246544  huu\n",
            "tensor(0.3323, grad_fn=<NllLossBackward0>) lol\n",
            "8.671817883849144  huu\n",
            "tensor(0.3517, grad_fn=<NllLossBackward0>) lol\n",
            "9.023542627692223  huu\n",
            "tensor(0.3078, grad_fn=<NllLossBackward0>) lol\n",
            "9.331309124827385  huu\n",
            "tensor(0.2592, grad_fn=<NllLossBackward0>) lol\n",
            "9.590499266982079  huu\n",
            "tensor(0.2408, grad_fn=<NllLossBackward0>) lol\n",
            "9.831329748034477  huu\n",
            "tensor(0.2029, grad_fn=<NllLossBackward0>) lol\n",
            "10.03422436118126  huu\n",
            "tensor(0.2539, grad_fn=<NllLossBackward0>) lol\n",
            "10.28812575340271  huu\n",
            "tensor(0.2741, grad_fn=<NllLossBackward0>) lol\n",
            "10.562229007482529  huu\n",
            "tensor(0.3477, grad_fn=<NllLossBackward0>) lol\n",
            "10.909966826438904  huu\n",
            "tensor(0.1523, grad_fn=<NllLossBackward0>) lol\n",
            "11.062223702669144  huu\n",
            "tensor(0.2247, grad_fn=<NllLossBackward0>) lol\n",
            "11.2869433760643  huu\n",
            "tensor(0.2245, grad_fn=<NllLossBackward0>) lol\n",
            "11.511480286717415  huu\n",
            "tensor(0.1897, grad_fn=<NllLossBackward0>) lol\n",
            "11.701132729649544  huu\n",
            "tensor(0.2906, grad_fn=<NllLossBackward0>) lol\n",
            "11.991780295968056  huu\n",
            "tensor(0.3146, grad_fn=<NllLossBackward0>) lol\n",
            "12.306340679526329  huu\n",
            "tensor(0.2338, grad_fn=<NllLossBackward0>) lol\n",
            "12.540148258209229  huu\n",
            "tensor(0.2471, grad_fn=<NllLossBackward0>) lol\n",
            "12.787239953875542  huu\n",
            "tensor(0.2426, grad_fn=<NllLossBackward0>) lol\n",
            "13.029881805181503  huu\n",
            "tensor(0.2025, grad_fn=<NllLossBackward0>) lol\n",
            "13.232372432947159  huu\n",
            "tensor(0.2244, grad_fn=<NllLossBackward0>) lol\n",
            "13.456769734621048  huu\n",
            "tensor(0.2182, grad_fn=<NllLossBackward0>) lol\n",
            "13.675017416477203  huu\n",
            "tensor(0.1987, grad_fn=<NllLossBackward0>) lol\n",
            "13.873690575361252  huu\n",
            "tensor(0.2251, grad_fn=<NllLossBackward0>) lol\n",
            "14.098832711577415  huu\n",
            "tensor(0.3921, grad_fn=<NllLossBackward0>) lol\n",
            "14.490931406617165  huu\n",
            "tensor(0.3029, grad_fn=<NllLossBackward0>) lol\n",
            "14.793869212269783  huu\n",
            "tensor(0.1860, grad_fn=<NllLossBackward0>) lol\n",
            "14.979889869689941  huu\n",
            "tensor(0.2193, grad_fn=<NllLossBackward0>) lol\n",
            "15.199164792895317  huu\n",
            "tensor(0.2037, grad_fn=<NllLossBackward0>) lol\n",
            "15.402815997600555  huu\n",
            "tensor(0.1808, grad_fn=<NllLossBackward0>) lol\n",
            "15.583571121096611  huu\n",
            "tensor(0.2400, grad_fn=<NllLossBackward0>) lol\n",
            "15.823609039187431  huu\n",
            "tensor(0.1799, grad_fn=<NllLossBackward0>) lol\n",
            "16.003484323620796  huu\n",
            "tensor(0.1936, grad_fn=<NllLossBackward0>) lol\n",
            "16.197103396058083  huu\n",
            "tensor(0.2894, grad_fn=<NllLossBackward0>) lol\n",
            "16.486552760004997  huu\n",
            "tensor(0.1798, grad_fn=<NllLossBackward0>) lol\n",
            "16.666303798556328  huu\n",
            "tensor(0.2004, grad_fn=<NllLossBackward0>) lol\n",
            "16.866721376776695  huu\n",
            "tensor(0.2198, grad_fn=<NllLossBackward0>) lol\n",
            "17.086484223604202  huu\n",
            "tensor(0.1985, grad_fn=<NllLossBackward0>) lol\n",
            "17.28495194017887  huu\n",
            "tensor(0.3319, grad_fn=<NllLossBackward0>) lol\n",
            "17.616845205426216  huu\n",
            "tensor(0.1872, grad_fn=<NllLossBackward0>) lol\n",
            "17.804071590304375  huu\n",
            "tensor(0.1523, grad_fn=<NllLossBackward0>) lol\n",
            "17.95639018714428  huu\n",
            "tensor(0.2037, grad_fn=<NllLossBackward0>) lol\n",
            "18.16007474064827  huu\n",
            "tensor(0.1342, grad_fn=<NllLossBackward0>) lol\n",
            "18.294259011745453  huu\n",
            "tensor(0.1460, grad_fn=<NllLossBackward0>) lol\n",
            "18.44028550386429  huu\n",
            "tensor(0.4110, grad_fn=<NllLossBackward0>) lol\n",
            "18.851295828819275  huu\n",
            "tensor(0.2599, grad_fn=<NllLossBackward0>) lol\n",
            "19.111204594373703  huu\n",
            "tensor(0.2962, grad_fn=<NllLossBackward0>) lol\n",
            "19.407370567321777  huu\n",
            "tensor(0.3184, grad_fn=<NllLossBackward0>) lol\n",
            "19.72573956847191  huu\n",
            "tensor(0.1958, grad_fn=<NllLossBackward0>) lol\n",
            "19.921531826257706  huu\n",
            "tensor(0.2028, grad_fn=<NllLossBackward0>) lol\n",
            "20.124340891838074  huu\n",
            "epoch : 5/10, recon loss = 0.24541879\n",
            "tensor(0.2032, grad_fn=<NllLossBackward0>) lol\n",
            "0.20322713255882263  huu\n",
            "tensor(0.3059, grad_fn=<NllLossBackward0>) lol\n",
            "0.5091044008731842  huu\n",
            "tensor(0.2386, grad_fn=<NllLossBackward0>) lol\n",
            "0.747726246714592  huu\n",
            "tensor(0.2806, grad_fn=<NllLossBackward0>) lol\n",
            "1.0283599942922592  huu\n",
            "tensor(0.2160, grad_fn=<NllLossBackward0>) lol\n",
            "1.2443423122167587  huu\n",
            "tensor(0.2644, grad_fn=<NllLossBackward0>) lol\n",
            "1.5087838917970657  huu\n",
            "tensor(0.3214, grad_fn=<NllLossBackward0>) lol\n",
            "1.8301596194505692  huu\n",
            "tensor(0.2535, grad_fn=<NllLossBackward0>) lol\n",
            "2.08365561068058  huu\n",
            "tensor(0.1441, grad_fn=<NllLossBackward0>) lol\n",
            "2.2277554720640182  huu\n",
            "tensor(0.2007, grad_fn=<NllLossBackward0>) lol\n",
            "2.428459271788597  huu\n",
            "tensor(0.2625, grad_fn=<NllLossBackward0>) lol\n",
            "2.690956398844719  huu\n",
            "tensor(0.1931, grad_fn=<NllLossBackward0>) lol\n",
            "2.884040892124176  huu\n",
            "tensor(0.2974, grad_fn=<NllLossBackward0>) lol\n",
            "3.1814061105251312  huu\n",
            "tensor(0.2512, grad_fn=<NllLossBackward0>) lol\n",
            "3.432632178068161  huu\n",
            "tensor(0.2470, grad_fn=<NllLossBackward0>) lol\n",
            "3.679664760828018  huu\n",
            "tensor(0.1726, grad_fn=<NllLossBackward0>) lol\n",
            "3.8522520810365677  huu\n",
            "tensor(0.1728, grad_fn=<NllLossBackward0>) lol\n",
            "4.025098443031311  huu\n",
            "tensor(0.2071, grad_fn=<NllLossBackward0>) lol\n",
            "4.232171878218651  huu\n",
            "tensor(0.3463, grad_fn=<NllLossBackward0>) lol\n",
            "4.578482761979103  huu\n",
            "tensor(0.2995, grad_fn=<NllLossBackward0>) lol\n",
            "4.877984032034874  huu\n",
            "tensor(0.2462, grad_fn=<NllLossBackward0>) lol\n",
            "5.124135613441467  huu\n",
            "tensor(0.1893, grad_fn=<NllLossBackward0>) lol\n",
            "5.31347481906414  huu\n",
            "tensor(0.1457, grad_fn=<NllLossBackward0>) lol\n",
            "5.45915649831295  huu\n",
            "tensor(0.2554, grad_fn=<NllLossBackward0>) lol\n",
            "5.714539512991905  huu\n",
            "tensor(0.2057, grad_fn=<NllLossBackward0>) lol\n",
            "5.920272514224052  huu\n",
            "tensor(0.2022, grad_fn=<NllLossBackward0>) lol\n",
            "6.122448816895485  huu\n",
            "tensor(0.2029, grad_fn=<NllLossBackward0>) lol\n",
            "6.325391948223114  huu\n",
            "tensor(0.1606, grad_fn=<NllLossBackward0>) lol\n",
            "6.485987514257431  huu\n",
            "tensor(0.2549, grad_fn=<NllLossBackward0>) lol\n",
            "6.740850120782852  huu\n",
            "tensor(0.2183, grad_fn=<NllLossBackward0>) lol\n",
            "6.959163352847099  huu\n",
            "tensor(0.1204, grad_fn=<NllLossBackward0>) lol\n",
            "7.079581253230572  huu\n",
            "tensor(0.2175, grad_fn=<NllLossBackward0>) lol\n",
            "7.297077976167202  huu\n",
            "tensor(0.1444, grad_fn=<NllLossBackward0>) lol\n",
            "7.441483847796917  huu\n",
            "tensor(0.1761, grad_fn=<NllLossBackward0>) lol\n",
            "7.617588855326176  huu\n",
            "tensor(0.1601, grad_fn=<NllLossBackward0>) lol\n",
            "7.777657113969326  huu\n",
            "tensor(0.3132, grad_fn=<NllLossBackward0>) lol\n",
            "8.090881191194057  huu\n",
            "tensor(0.2198, grad_fn=<NllLossBackward0>) lol\n",
            "8.31071550399065  huu\n",
            "tensor(0.2009, grad_fn=<NllLossBackward0>) lol\n",
            "8.51158145815134  huu\n",
            "tensor(0.2023, grad_fn=<NllLossBackward0>) lol\n",
            "8.713921047747135  huu\n",
            "tensor(0.2513, grad_fn=<NllLossBackward0>) lol\n",
            "8.965194292366505  huu\n",
            "tensor(0.1426, grad_fn=<NllLossBackward0>) lol\n",
            "9.107785262167454  huu\n",
            "tensor(0.4226, grad_fn=<NllLossBackward0>) lol\n",
            "9.530352510511875  huu\n",
            "tensor(0.2113, grad_fn=<NllLossBackward0>) lol\n",
            "9.741701178252697  huu\n",
            "tensor(0.3056, grad_fn=<NllLossBackward0>) lol\n",
            "10.047275893390179  huu\n",
            "tensor(0.2821, grad_fn=<NllLossBackward0>) lol\n",
            "10.329376809298992  huu\n",
            "tensor(0.2517, grad_fn=<NllLossBackward0>) lol\n",
            "10.581035159528255  huu\n",
            "tensor(0.1471, grad_fn=<NllLossBackward0>) lol\n",
            "10.728147350251675  huu\n",
            "tensor(0.2787, grad_fn=<NllLossBackward0>) lol\n",
            "11.006862543523312  huu\n",
            "tensor(0.1669, grad_fn=<NllLossBackward0>) lol\n",
            "11.173742897808552  huu\n",
            "tensor(0.2451, grad_fn=<NllLossBackward0>) lol\n",
            "11.418807245790958  huu\n",
            "tensor(0.2486, grad_fn=<NllLossBackward0>) lol\n",
            "11.667429126799107  huu\n",
            "tensor(0.2084, grad_fn=<NllLossBackward0>) lol\n",
            "11.87586273998022  huu\n",
            "tensor(0.2901, grad_fn=<NllLossBackward0>) lol\n",
            "12.166007660329342  huu\n",
            "tensor(0.1798, grad_fn=<NllLossBackward0>) lol\n",
            "12.345820136368275  huu\n",
            "tensor(0.1525, grad_fn=<NllLossBackward0>) lol\n",
            "12.498335592448711  huu\n",
            "tensor(0.2327, grad_fn=<NllLossBackward0>) lol\n",
            "12.731053464114666  huu\n",
            "tensor(0.1722, grad_fn=<NllLossBackward0>) lol\n",
            "12.903302870690823  huu\n",
            "tensor(0.1472, grad_fn=<NllLossBackward0>) lol\n",
            "13.050489820539951  huu\n",
            "tensor(0.3177, grad_fn=<NllLossBackward0>) lol\n",
            "13.368186987936497  huu\n",
            "tensor(0.2572, grad_fn=<NllLossBackward0>) lol\n",
            "13.62537119537592  huu\n",
            "tensor(0.2747, grad_fn=<NllLossBackward0>) lol\n",
            "13.900048770010471  huu\n",
            "tensor(0.2372, grad_fn=<NllLossBackward0>) lol\n",
            "14.137238062918186  huu\n",
            "tensor(0.2571, grad_fn=<NllLossBackward0>) lol\n",
            "14.394323982298374  huu\n",
            "tensor(0.2499, grad_fn=<NllLossBackward0>) lol\n",
            "14.64424716681242  huu\n",
            "tensor(0.2629, grad_fn=<NllLossBackward0>) lol\n",
            "14.907108716666698  huu\n",
            "tensor(0.1391, grad_fn=<NllLossBackward0>) lol\n",
            "15.046182699501514  huu\n",
            "tensor(0.1684, grad_fn=<NllLossBackward0>) lol\n",
            "15.21461222320795  huu\n",
            "tensor(0.2730, grad_fn=<NllLossBackward0>) lol\n",
            "15.487654395401478  huu\n",
            "tensor(0.3526, grad_fn=<NllLossBackward0>) lol\n",
            "15.840212352573872  huu\n",
            "tensor(0.2862, grad_fn=<NllLossBackward0>) lol\n",
            "16.126370526850224  huu\n",
            "tensor(0.1576, grad_fn=<NllLossBackward0>) lol\n",
            "16.28395015746355  huu\n",
            "tensor(0.2385, grad_fn=<NllLossBackward0>) lol\n",
            "16.522456251084805  huu\n",
            "tensor(0.3222, grad_fn=<NllLossBackward0>) lol\n",
            "16.844609908759594  huu\n",
            "tensor(0.2515, grad_fn=<NllLossBackward0>) lol\n",
            "17.096137337386608  huu\n",
            "tensor(0.2397, grad_fn=<NllLossBackward0>) lol\n",
            "17.33580841869116  huu\n",
            "tensor(0.1111, grad_fn=<NllLossBackward0>) lol\n",
            "17.446863919496536  huu\n",
            "tensor(0.3640, grad_fn=<NllLossBackward0>) lol\n",
            "17.81089422106743  huu\n",
            "tensor(0.2141, grad_fn=<NllLossBackward0>) lol\n",
            "18.024985298514366  huu\n",
            "tensor(0.2104, grad_fn=<NllLossBackward0>) lol\n",
            "18.23540136218071  huu\n",
            "tensor(0.2411, grad_fn=<NllLossBackward0>) lol\n",
            "18.476538091897964  huu\n",
            "tensor(0.2482, grad_fn=<NllLossBackward0>) lol\n",
            "18.72470174729824  huu\n",
            "tensor(0.2528, grad_fn=<NllLossBackward0>) lol\n",
            "18.977462366223335  huu\n",
            "epoch : 6/10, recon loss = 0.23143247\n",
            "tensor(0.2731, grad_fn=<NllLossBackward0>) lol\n",
            "0.2730812728404999  huu\n",
            "tensor(0.2168, grad_fn=<NllLossBackward0>) lol\n",
            "0.48986461758613586  huu\n",
            "tensor(0.2505, grad_fn=<NllLossBackward0>) lol\n",
            "0.7404084205627441  huu\n",
            "tensor(0.2003, grad_fn=<NllLossBackward0>) lol\n",
            "0.9407395422458649  huu\n",
            "tensor(0.4456, grad_fn=<NllLossBackward0>) lol\n",
            "1.386374980211258  huu\n",
            "tensor(0.2809, grad_fn=<NllLossBackward0>) lol\n",
            "1.667234480381012  huu\n",
            "tensor(0.1800, grad_fn=<NllLossBackward0>) lol\n",
            "1.847225844860077  huu\n",
            "tensor(0.2114, grad_fn=<NllLossBackward0>) lol\n",
            "2.0586628168821335  huu\n",
            "tensor(0.3472, grad_fn=<NllLossBackward0>) lol\n",
            "2.405856803059578  huu\n",
            "tensor(0.2488, grad_fn=<NllLossBackward0>) lol\n",
            "2.654670864343643  huu\n",
            "tensor(0.1990, grad_fn=<NllLossBackward0>) lol\n",
            "2.8536643385887146  huu\n",
            "tensor(0.1753, grad_fn=<NllLossBackward0>) lol\n",
            "3.028974324464798  huu\n",
            "tensor(0.3646, grad_fn=<NllLossBackward0>) lol\n",
            "3.3935439586639404  huu\n",
            "tensor(0.2978, grad_fn=<NllLossBackward0>) lol\n",
            "3.6913250386714935  huu\n",
            "tensor(0.1857, grad_fn=<NllLossBackward0>) lol\n",
            "3.876981019973755  huu\n",
            "tensor(0.2857, grad_fn=<NllLossBackward0>) lol\n",
            "4.162726283073425  huu\n",
            "tensor(0.2201, grad_fn=<NllLossBackward0>) lol\n",
            "4.38287578523159  huu\n",
            "tensor(0.2449, grad_fn=<NllLossBackward0>) lol\n",
            "4.627816140651703  huu\n",
            "tensor(0.2511, grad_fn=<NllLossBackward0>) lol\n",
            "4.878884702920914  huu\n",
            "tensor(0.1673, grad_fn=<NllLossBackward0>) lol\n",
            "5.04619100689888  huu\n",
            "tensor(0.1625, grad_fn=<NllLossBackward0>) lol\n",
            "5.208713337779045  huu\n",
            "tensor(0.1910, grad_fn=<NllLossBackward0>) lol\n",
            "5.399682655930519  huu\n",
            "tensor(0.1737, grad_fn=<NllLossBackward0>) lol\n",
            "5.573425769805908  huu\n",
            "tensor(0.1859, grad_fn=<NllLossBackward0>) lol\n",
            "5.759317547082901  huu\n",
            "tensor(0.2989, grad_fn=<NllLossBackward0>) lol\n",
            "6.058181345462799  huu\n",
            "tensor(0.3680, grad_fn=<NllLossBackward0>) lol\n",
            "6.426184356212616  huu\n",
            "tensor(0.1797, grad_fn=<NllLossBackward0>) lol\n",
            "6.6058665961027145  huu\n",
            "tensor(0.1513, grad_fn=<NllLossBackward0>) lol\n",
            "6.75716595351696  huu\n",
            "tensor(0.1727, grad_fn=<NllLossBackward0>) lol\n",
            "6.929901376366615  huu\n",
            "tensor(0.2111, grad_fn=<NllLossBackward0>) lol\n",
            "7.141014635562897  huu\n",
            "tensor(0.1657, grad_fn=<NllLossBackward0>) lol\n",
            "7.306710436940193  huu\n",
            "tensor(0.1922, grad_fn=<NllLossBackward0>) lol\n",
            "7.4989140927791595  huu\n",
            "tensor(0.1883, grad_fn=<NllLossBackward0>) lol\n",
            "7.687240183353424  huu\n",
            "tensor(0.1714, grad_fn=<NllLossBackward0>) lol\n",
            "7.858662337064743  huu\n",
            "tensor(0.2713, grad_fn=<NllLossBackward0>) lol\n",
            "8.129971265792847  huu\n",
            "tensor(0.2709, grad_fn=<NllLossBackward0>) lol\n",
            "8.400908350944519  huu\n",
            "tensor(0.1619, grad_fn=<NllLossBackward0>) lol\n",
            "8.56283862888813  huu\n",
            "tensor(0.2527, grad_fn=<NllLossBackward0>) lol\n",
            "8.815527334809303  huu\n",
            "tensor(0.1538, grad_fn=<NllLossBackward0>) lol\n",
            "8.969317242503166  huu\n",
            "tensor(0.2517, grad_fn=<NllLossBackward0>) lol\n",
            "9.220977738499641  huu\n",
            "tensor(0.1485, grad_fn=<NllLossBackward0>) lol\n",
            "9.369503766298294  huu\n",
            "tensor(0.1754, grad_fn=<NllLossBackward0>) lol\n",
            "9.544889435172081  huu\n",
            "tensor(0.1922, grad_fn=<NllLossBackward0>) lol\n",
            "9.737060353159904  huu\n",
            "tensor(0.2881, grad_fn=<NllLossBackward0>) lol\n",
            "10.025113210082054  huu\n",
            "tensor(0.1968, grad_fn=<NllLossBackward0>) lol\n",
            "10.221942007541656  huu\n",
            "tensor(0.2489, grad_fn=<NllLossBackward0>) lol\n",
            "10.470836147665977  huu\n",
            "tensor(0.2709, grad_fn=<NllLossBackward0>) lol\n",
            "10.741702660918236  huu\n",
            "tensor(0.1222, grad_fn=<NllLossBackward0>) lol\n",
            "10.863894335925579  huu\n",
            "tensor(0.2929, grad_fn=<NllLossBackward0>) lol\n",
            "11.156790874898434  huu\n",
            "tensor(0.2032, grad_fn=<NllLossBackward0>) lol\n",
            "11.36000131815672  huu\n",
            "tensor(0.2598, grad_fn=<NllLossBackward0>) lol\n",
            "11.61977805942297  huu\n",
            "tensor(0.1617, grad_fn=<NllLossBackward0>) lol\n",
            "11.781517632305622  huu\n",
            "tensor(0.1916, grad_fn=<NllLossBackward0>) lol\n",
            "11.973093755543232  huu\n",
            "tensor(0.1721, grad_fn=<NllLossBackward0>) lol\n",
            "12.145172782242298  huu\n",
            "tensor(0.2513, grad_fn=<NllLossBackward0>) lol\n",
            "12.396495647728443  huu\n",
            "tensor(0.2832, grad_fn=<NllLossBackward0>) lol\n",
            "12.679684109985828  huu\n",
            "tensor(0.1795, grad_fn=<NllLossBackward0>) lol\n",
            "12.859199859201908  huu\n",
            "tensor(0.1585, grad_fn=<NllLossBackward0>) lol\n",
            "13.017702691257  huu\n",
            "tensor(0.2174, grad_fn=<NllLossBackward0>) lol\n",
            "13.235127232968807  huu\n",
            "tensor(0.1207, grad_fn=<NllLossBackward0>) lol\n",
            "13.355779603123665  huu\n",
            "tensor(0.2047, grad_fn=<NllLossBackward0>) lol\n",
            "13.560458019375801  huu\n",
            "tensor(0.1637, grad_fn=<NllLossBackward0>) lol\n",
            "13.724176421761513  huu\n",
            "tensor(0.1733, grad_fn=<NllLossBackward0>) lol\n",
            "13.897485941648483  huu\n",
            "tensor(0.2630, grad_fn=<NllLossBackward0>) lol\n",
            "14.160460114479065  huu\n",
            "tensor(0.2344, grad_fn=<NllLossBackward0>) lol\n",
            "14.394886434078217  huu\n",
            "tensor(0.2207, grad_fn=<NllLossBackward0>) lol\n",
            "14.615587592124939  huu\n",
            "tensor(0.1960, grad_fn=<NllLossBackward0>) lol\n",
            "14.811619848012924  huu\n",
            "tensor(0.3271, grad_fn=<NllLossBackward0>) lol\n",
            "15.138682067394257  huu\n",
            "tensor(0.4080, grad_fn=<NllLossBackward0>) lol\n",
            "15.546731919050217  huu\n",
            "tensor(0.1807, grad_fn=<NllLossBackward0>) lol\n",
            "15.72745369374752  huu\n",
            "tensor(0.1521, grad_fn=<NllLossBackward0>) lol\n",
            "15.879537239670753  huu\n",
            "tensor(0.3545, grad_fn=<NllLossBackward0>) lol\n",
            "16.234026446938515  huu\n",
            "tensor(0.1712, grad_fn=<NllLossBackward0>) lol\n",
            "16.405184894800186  huu\n",
            "tensor(0.2458, grad_fn=<NllLossBackward0>) lol\n",
            "16.65098586678505  huu\n",
            "tensor(0.2472, grad_fn=<NllLossBackward0>) lol\n",
            "16.898226752877235  huu\n",
            "tensor(0.2879, grad_fn=<NllLossBackward0>) lol\n",
            "17.186132684350014  huu\n",
            "tensor(0.1506, grad_fn=<NllLossBackward0>) lol\n",
            "17.336759775877  huu\n",
            "tensor(0.2416, grad_fn=<NllLossBackward0>) lol\n",
            "17.57833380997181  huu\n",
            "tensor(0.1520, grad_fn=<NllLossBackward0>) lol\n",
            "17.730369687080383  huu\n",
            "tensor(0.1620, grad_fn=<NllLossBackward0>) lol\n",
            "17.892410814762115  huu\n",
            "tensor(0.2105, grad_fn=<NllLossBackward0>) lol\n",
            "18.102896228432655  huu\n",
            "tensor(0.2310, grad_fn=<NllLossBackward0>) lol\n",
            "18.333898603916168  huu\n",
            "epoch : 7/10, recon loss = 0.22358413\n",
            "tensor(0.1701, grad_fn=<NllLossBackward0>) lol\n",
            "0.17009639739990234  huu\n",
            "tensor(0.2983, grad_fn=<NllLossBackward0>) lol\n",
            "0.4683525264263153  huu\n",
            "tensor(0.1948, grad_fn=<NllLossBackward0>) lol\n",
            "0.6631941944360733  huu\n",
            "tensor(0.1205, grad_fn=<NllLossBackward0>) lol\n",
            "0.7837375476956367  huu\n",
            "tensor(0.2749, grad_fn=<NllLossBackward0>) lol\n",
            "1.0585947707295418  huu\n",
            "tensor(0.2068, grad_fn=<NllLossBackward0>) lol\n",
            "1.2653856351971626  huu\n",
            "tensor(0.2347, grad_fn=<NllLossBackward0>) lol\n",
            "1.5000470355153084  huu\n",
            "tensor(0.3287, grad_fn=<NllLossBackward0>) lol\n",
            "1.8287010863423347  huu\n",
            "tensor(0.1565, grad_fn=<NllLossBackward0>) lol\n",
            "1.9852176383137703  huu\n",
            "tensor(0.1597, grad_fn=<NllLossBackward0>) lol\n",
            "2.144879974424839  huu\n",
            "tensor(0.1742, grad_fn=<NllLossBackward0>) lol\n",
            "2.3190764263272285  huu\n",
            "tensor(0.3005, grad_fn=<NllLossBackward0>) lol\n",
            "2.6195621863007545  huu\n",
            "tensor(0.3061, grad_fn=<NllLossBackward0>) lol\n",
            "2.9256538823246956  huu\n",
            "tensor(0.1698, grad_fn=<NllLossBackward0>) lol\n",
            "3.095502533018589  huu\n",
            "tensor(0.1080, grad_fn=<NllLossBackward0>) lol\n",
            "3.2035290598869324  huu\n",
            "tensor(0.1905, grad_fn=<NllLossBackward0>) lol\n",
            "3.3940412998199463  huu\n",
            "tensor(0.2840, grad_fn=<NllLossBackward0>) lol\n",
            "3.678059160709381  huu\n",
            "tensor(0.2655, grad_fn=<NllLossBackward0>) lol\n",
            "3.9435295164585114  huu\n",
            "tensor(0.2188, grad_fn=<NllLossBackward0>) lol\n",
            "4.1622979789972305  huu\n",
            "tensor(0.1607, grad_fn=<NllLossBackward0>) lol\n",
            "4.323029577732086  huu\n",
            "tensor(0.2378, grad_fn=<NllLossBackward0>) lol\n",
            "4.560807466506958  huu\n",
            "tensor(0.3464, grad_fn=<NllLossBackward0>) lol\n",
            "4.907220542430878  huu\n",
            "tensor(0.1994, grad_fn=<NllLossBackward0>) lol\n",
            "5.106660395860672  huu\n",
            "tensor(0.1777, grad_fn=<NllLossBackward0>) lol\n",
            "5.284367054700851  huu\n",
            "tensor(0.1647, grad_fn=<NllLossBackward0>) lol\n",
            "5.449085131287575  huu\n",
            "tensor(0.2460, grad_fn=<NllLossBackward0>) lol\n",
            "5.695070460438728  huu\n",
            "tensor(0.1974, grad_fn=<NllLossBackward0>) lol\n",
            "5.892450734972954  huu\n",
            "tensor(0.1384, grad_fn=<NllLossBackward0>) lol\n",
            "6.030896902084351  huu\n",
            "tensor(0.2537, grad_fn=<NllLossBackward0>) lol\n",
            "6.284605801105499  huu\n",
            "tensor(0.2044, grad_fn=<NllLossBackward0>) lol\n",
            "6.489050954580307  huu\n",
            "tensor(0.1661, grad_fn=<NllLossBackward0>) lol\n",
            "6.655182600021362  huu\n",
            "tensor(0.1802, grad_fn=<NllLossBackward0>) lol\n",
            "6.835350468754768  huu\n",
            "tensor(0.1510, grad_fn=<NllLossBackward0>) lol\n",
            "6.986341014504433  huu\n",
            "tensor(0.1211, grad_fn=<NllLossBackward0>) lol\n",
            "7.107488758862019  huu\n",
            "tensor(0.3714, grad_fn=<NllLossBackward0>) lol\n",
            "7.478932894766331  huu\n",
            "tensor(0.2655, grad_fn=<NllLossBackward0>) lol\n",
            "7.74445117264986  huu\n",
            "tensor(0.1159, grad_fn=<NllLossBackward0>) lol\n",
            "7.8603281155228615  huu\n",
            "tensor(0.1497, grad_fn=<NllLossBackward0>) lol\n",
            "8.010038949549198  huu\n",
            "tensor(0.2305, grad_fn=<NllLossBackward0>) lol\n",
            "8.240519739687443  huu\n",
            "tensor(0.1836, grad_fn=<NllLossBackward0>) lol\n",
            "8.424074046313763  huu\n",
            "tensor(0.1677, grad_fn=<NllLossBackward0>) lol\n",
            "8.591743551194668  huu\n",
            "tensor(0.1446, grad_fn=<NllLossBackward0>) lol\n",
            "8.736379034817219  huu\n",
            "tensor(0.3082, grad_fn=<NllLossBackward0>) lol\n",
            "9.044544942677021  huu\n",
            "tensor(0.2555, grad_fn=<NllLossBackward0>) lol\n",
            "9.300042070448399  huu\n",
            "tensor(0.2304, grad_fn=<NllLossBackward0>) lol\n",
            "9.530392967164516  huu\n",
            "tensor(0.2381, grad_fn=<NllLossBackward0>) lol\n",
            "9.768447302281857  huu\n",
            "tensor(0.1553, grad_fn=<NllLossBackward0>) lol\n",
            "9.923711217939854  huu\n",
            "tensor(0.1107, grad_fn=<NllLossBackward0>) lol\n",
            "10.034418568015099  huu\n",
            "tensor(0.2967, grad_fn=<NllLossBackward0>) lol\n",
            "10.331162706017494  huu\n",
            "tensor(0.1580, grad_fn=<NllLossBackward0>) lol\n",
            "10.489143177866936  huu\n",
            "tensor(0.2195, grad_fn=<NllLossBackward0>) lol\n",
            "10.70861579477787  huu\n",
            "tensor(0.1372, grad_fn=<NllLossBackward0>) lol\n",
            "10.845817655324936  huu\n",
            "tensor(0.2398, grad_fn=<NllLossBackward0>) lol\n",
            "11.085620820522308  huu\n",
            "tensor(0.2114, grad_fn=<NllLossBackward0>) lol\n",
            "11.297065138816833  huu\n",
            "tensor(0.2753, grad_fn=<NllLossBackward0>) lol\n",
            "11.572316288948059  huu\n",
            "tensor(0.2920, grad_fn=<NllLossBackward0>) lol\n",
            "11.864334791898727  huu\n",
            "tensor(0.2344, grad_fn=<NllLossBackward0>) lol\n",
            "12.098691940307617  huu\n",
            "tensor(0.3097, grad_fn=<NllLossBackward0>) lol\n",
            "12.408366560935974  huu\n",
            "tensor(0.2085, grad_fn=<NllLossBackward0>) lol\n",
            "12.616862386465073  huu\n",
            "tensor(0.1663, grad_fn=<NllLossBackward0>) lol\n",
            "12.783137932419777  huu\n",
            "tensor(0.2183, grad_fn=<NllLossBackward0>) lol\n",
            "13.001399040222168  huu\n",
            "tensor(0.2089, grad_fn=<NllLossBackward0>) lol\n",
            "13.210329174995422  huu\n",
            "tensor(0.2141, grad_fn=<NllLossBackward0>) lol\n",
            "13.424424976110458  huu\n",
            "tensor(0.1480, grad_fn=<NllLossBackward0>) lol\n",
            "13.572392627596855  huu\n",
            "tensor(0.2001, grad_fn=<NllLossBackward0>) lol\n",
            "13.772520959377289  huu\n",
            "tensor(0.2843, grad_fn=<NllLossBackward0>) lol\n",
            "14.05682435631752  huu\n",
            "tensor(0.3109, grad_fn=<NllLossBackward0>) lol\n",
            "14.36769288778305  huu\n",
            "tensor(0.2089, grad_fn=<NllLossBackward0>) lol\n",
            "14.57664155960083  huu\n",
            "tensor(0.1552, grad_fn=<NllLossBackward0>) lol\n",
            "14.731827557086945  huu\n",
            "tensor(0.1642, grad_fn=<NllLossBackward0>) lol\n",
            "14.896016284823418  huu\n",
            "tensor(0.2355, grad_fn=<NllLossBackward0>) lol\n",
            "15.131488621234894  huu\n",
            "tensor(0.0997, grad_fn=<NllLossBackward0>) lol\n",
            "15.23122901469469  huu\n",
            "tensor(0.2022, grad_fn=<NllLossBackward0>) lol\n",
            "15.433478496968746  huu\n",
            "tensor(0.2025, grad_fn=<NllLossBackward0>) lol\n",
            "15.63598208874464  huu\n",
            "tensor(0.1177, grad_fn=<NllLossBackward0>) lol\n",
            "15.753677494823933  huu\n",
            "tensor(0.3348, grad_fn=<NllLossBackward0>) lol\n",
            "16.08851931244135  huu\n",
            "tensor(0.3488, grad_fn=<NllLossBackward0>) lol\n",
            "16.43730089813471  huu\n",
            "tensor(0.2462, grad_fn=<NllLossBackward0>) lol\n",
            "16.683504231274128  huu\n",
            "tensor(0.2891, grad_fn=<NllLossBackward0>) lol\n",
            "16.972614653408527  huu\n",
            "tensor(0.3009, grad_fn=<NllLossBackward0>) lol\n",
            "17.273514337837696  huu\n",
            "tensor(0.2876, grad_fn=<NllLossBackward0>) lol\n",
            "17.56108758598566  huu\n",
            "tensor(0.3042, grad_fn=<NllLossBackward0>) lol\n",
            "17.865259028971195  huu\n",
            "epoch : 8/10, recon loss = 0.21786901\n",
            "tensor(0.2078, grad_fn=<NllLossBackward0>) lol\n",
            "0.20782332122325897  huu\n",
            "tensor(0.1206, grad_fn=<NllLossBackward0>) lol\n",
            "0.3283991515636444  huu\n",
            "tensor(0.2646, grad_fn=<NllLossBackward0>) lol\n",
            "0.5930397212505341  huu\n",
            "tensor(0.2287, grad_fn=<NllLossBackward0>) lol\n",
            "0.8217243403196335  huu\n",
            "tensor(0.1360, grad_fn=<NllLossBackward0>) lol\n",
            "0.9577179104089737  huu\n",
            "tensor(0.1034, grad_fn=<NllLossBackward0>) lol\n",
            "1.061082437634468  huu\n",
            "tensor(0.2315, grad_fn=<NllLossBackward0>) lol\n",
            "1.2925560772418976  huu\n",
            "tensor(0.2433, grad_fn=<NllLossBackward0>) lol\n",
            "1.5358400493860245  huu\n",
            "tensor(0.3070, grad_fn=<NllLossBackward0>) lol\n",
            "1.8428090959787369  huu\n",
            "tensor(0.1556, grad_fn=<NllLossBackward0>) lol\n",
            "1.9983942806720734  huu\n",
            "tensor(0.2903, grad_fn=<NllLossBackward0>) lol\n",
            "2.28866907954216  huu\n",
            "tensor(0.2951, grad_fn=<NllLossBackward0>) lol\n",
            "2.583730101585388  huu\n",
            "tensor(0.2462, grad_fn=<NllLossBackward0>) lol\n",
            "2.829916551709175  huu\n",
            "tensor(0.2330, grad_fn=<NllLossBackward0>) lol\n",
            "3.06290403008461  huu\n",
            "tensor(0.2793, grad_fn=<NllLossBackward0>) lol\n",
            "3.3421911001205444  huu\n",
            "tensor(0.2153, grad_fn=<NllLossBackward0>) lol\n",
            "3.557527482509613  huu\n",
            "tensor(0.1649, grad_fn=<NllLossBackward0>) lol\n",
            "3.722427859902382  huu\n",
            "tensor(0.2323, grad_fn=<NllLossBackward0>) lol\n",
            "3.954761579632759  huu\n",
            "tensor(0.2601, grad_fn=<NllLossBackward0>) lol\n",
            "4.214857950806618  huu\n",
            "tensor(0.1683, grad_fn=<NllLossBackward0>) lol\n",
            "4.3831771314144135  huu\n",
            "tensor(0.1510, grad_fn=<NllLossBackward0>) lol\n",
            "4.534165918827057  huu\n",
            "tensor(0.1341, grad_fn=<NllLossBackward0>) lol\n",
            "4.668296962976456  huu\n",
            "tensor(0.2648, grad_fn=<NllLossBackward0>) lol\n",
            "4.933055996894836  huu\n",
            "tensor(0.2029, grad_fn=<NllLossBackward0>) lol\n",
            "5.135951727628708  huu\n",
            "tensor(0.2152, grad_fn=<NllLossBackward0>) lol\n",
            "5.351189702749252  huu\n",
            "tensor(0.1554, grad_fn=<NllLossBackward0>) lol\n",
            "5.5065698325634  huu\n",
            "tensor(0.1475, grad_fn=<NllLossBackward0>) lol\n",
            "5.654119312763214  huu\n",
            "tensor(0.2865, grad_fn=<NllLossBackward0>) lol\n",
            "5.940613180398941  huu\n",
            "tensor(0.1686, grad_fn=<NllLossBackward0>) lol\n",
            "6.109248772263527  huu\n",
            "tensor(0.2713, grad_fn=<NllLossBackward0>) lol\n",
            "6.380529448390007  huu\n",
            "tensor(0.2101, grad_fn=<NllLossBackward0>) lol\n",
            "6.590606272220612  huu\n",
            "tensor(0.2891, grad_fn=<NllLossBackward0>) lol\n",
            "6.879667550325394  huu\n",
            "tensor(0.1780, grad_fn=<NllLossBackward0>) lol\n",
            "7.057645171880722  huu\n",
            "tensor(0.3103, grad_fn=<NllLossBackward0>) lol\n",
            "7.367917478084564  huu\n",
            "tensor(0.1764, grad_fn=<NllLossBackward0>) lol\n",
            "7.544305831193924  huu\n",
            "tensor(0.2100, grad_fn=<NllLossBackward0>) lol\n",
            "7.75431926548481  huu\n",
            "tensor(0.2288, grad_fn=<NllLossBackward0>) lol\n",
            "7.98308552801609  huu\n",
            "tensor(0.1860, grad_fn=<NllLossBackward0>) lol\n",
            "8.169047519564629  huu\n",
            "tensor(0.1798, grad_fn=<NllLossBackward0>) lol\n",
            "8.34886571764946  huu\n",
            "tensor(0.1721, grad_fn=<NllLossBackward0>) lol\n",
            "8.521015331149101  huu\n",
            "tensor(0.2233, grad_fn=<NllLossBackward0>) lol\n",
            "8.744348779320717  huu\n",
            "tensor(0.2180, grad_fn=<NllLossBackward0>) lol\n",
            "8.962320759892464  huu\n",
            "tensor(0.2097, grad_fn=<NllLossBackward0>) lol\n",
            "9.172010362148285  huu\n",
            "tensor(0.1705, grad_fn=<NllLossBackward0>) lol\n",
            "9.342542588710785  huu\n",
            "tensor(0.2274, grad_fn=<NllLossBackward0>) lol\n",
            "9.5699393004179  huu\n",
            "tensor(0.1385, grad_fn=<NllLossBackward0>) lol\n",
            "9.708397597074509  huu\n",
            "tensor(0.2372, grad_fn=<NllLossBackward0>) lol\n",
            "9.945555463433266  huu\n",
            "tensor(0.2209, grad_fn=<NllLossBackward0>) lol\n",
            "10.16643875837326  huu\n",
            "tensor(0.1377, grad_fn=<NllLossBackward0>) lol\n",
            "10.304113239049911  huu\n",
            "tensor(0.2239, grad_fn=<NllLossBackward0>) lol\n",
            "10.527970358729362  huu\n",
            "tensor(0.1988, grad_fn=<NllLossBackward0>) lol\n",
            "10.72677731513977  huu\n",
            "tensor(0.2527, grad_fn=<NllLossBackward0>) lol\n",
            "10.97948145866394  huu\n",
            "tensor(0.2460, grad_fn=<NllLossBackward0>) lol\n",
            "11.225525066256523  huu\n",
            "tensor(0.3017, grad_fn=<NllLossBackward0>) lol\n",
            "11.52723915874958  huu\n",
            "tensor(0.2884, grad_fn=<NllLossBackward0>) lol\n",
            "11.81563301384449  huu\n",
            "tensor(0.2630, grad_fn=<NllLossBackward0>) lol\n",
            "12.078641220927238  huu\n",
            "tensor(0.2282, grad_fn=<NllLossBackward0>) lol\n",
            "12.30684132874012  huu\n",
            "tensor(0.3023, grad_fn=<NllLossBackward0>) lol\n",
            "12.609154984354973  huu\n",
            "tensor(0.1947, grad_fn=<NllLossBackward0>) lol\n",
            "12.803814485669136  huu\n",
            "tensor(0.1729, grad_fn=<NllLossBackward0>) lol\n",
            "12.976753666996956  huu\n",
            "tensor(0.2627, grad_fn=<NllLossBackward0>) lol\n",
            "13.239431694149971  huu\n",
            "tensor(0.2296, grad_fn=<NllLossBackward0>) lol\n",
            "13.469017937779427  huu\n",
            "tensor(0.1823, grad_fn=<NllLossBackward0>) lol\n",
            "13.651294723153114  huu\n",
            "tensor(0.2806, grad_fn=<NllLossBackward0>) lol\n",
            "13.931872352957726  huu\n",
            "tensor(0.1703, grad_fn=<NllLossBackward0>) lol\n",
            "14.102164670825005  huu\n",
            "tensor(0.1234, grad_fn=<NllLossBackward0>) lol\n",
            "14.225527621805668  huu\n",
            "tensor(0.3373, grad_fn=<NllLossBackward0>) lol\n",
            "14.562827534973621  huu\n",
            "tensor(0.2137, grad_fn=<NllLossBackward0>) lol\n",
            "14.776488058269024  huu\n",
            "tensor(0.1603, grad_fn=<NllLossBackward0>) lol\n",
            "14.936780609190464  huu\n",
            "tensor(0.2748, grad_fn=<NllLossBackward0>) lol\n",
            "15.211614526808262  huu\n",
            "tensor(0.0934, grad_fn=<NllLossBackward0>) lol\n",
            "15.30504460632801  huu\n",
            "tensor(0.1182, grad_fn=<NllLossBackward0>) lol\n",
            "15.423217058181763  huu\n",
            "tensor(0.1276, grad_fn=<NllLossBackward0>) lol\n",
            "15.550784021615982  huu\n",
            "tensor(0.2257, grad_fn=<NllLossBackward0>) lol\n",
            "15.776462987065315  huu\n",
            "tensor(0.2619, grad_fn=<NllLossBackward0>) lol\n",
            "16.038361117243767  huu\n",
            "tensor(0.2177, grad_fn=<NllLossBackward0>) lol\n",
            "16.256025925278664  huu\n",
            "tensor(0.2466, grad_fn=<NllLossBackward0>) lol\n",
            "16.50259481370449  huu\n",
            "tensor(0.1426, grad_fn=<NllLossBackward0>) lol\n",
            "16.645178571343422  huu\n",
            "tensor(0.2297, grad_fn=<NllLossBackward0>) lol\n",
            "16.874845325946808  huu\n",
            "tensor(0.2786, grad_fn=<NllLossBackward0>) lol\n",
            "17.153424471616745  huu\n",
            "tensor(0.2217, grad_fn=<NllLossBackward0>) lol\n",
            "17.3750791400671  huu\n",
            "tensor(0.1586, grad_fn=<NllLossBackward0>) lol\n",
            "17.53364124894142  huu\n",
            "epoch : 9/10, recon loss = 0.21382489\n",
            "tensor(0.3399, grad_fn=<NllLossBackward0>) lol\n",
            "0.33985453844070435  huu\n",
            "tensor(0.1529, grad_fn=<NllLossBackward0>) lol\n",
            "0.4927889108657837  huu\n",
            "tensor(0.1826, grad_fn=<NllLossBackward0>) lol\n",
            "0.6753794103860855  huu\n",
            "tensor(0.1897, grad_fn=<NllLossBackward0>) lol\n",
            "0.8650850653648376  huu\n",
            "tensor(0.0939, grad_fn=<NllLossBackward0>) lol\n",
            "0.9590044170618057  huu\n",
            "tensor(0.2288, grad_fn=<NllLossBackward0>) lol\n",
            "1.1878150254487991  huu\n",
            "tensor(0.1999, grad_fn=<NllLossBackward0>) lol\n",
            "1.3877185732126236  huu\n",
            "tensor(0.1865, grad_fn=<NllLossBackward0>) lol\n",
            "1.5742409974336624  huu\n",
            "tensor(0.2437, grad_fn=<NllLossBackward0>) lol\n",
            "1.8179797977209091  huu\n",
            "tensor(0.0884, grad_fn=<NllLossBackward0>) lol\n",
            "1.906378872692585  huu\n",
            "tensor(0.2574, grad_fn=<NllLossBackward0>) lol\n",
            "2.1637696996331215  huu\n",
            "tensor(0.1366, grad_fn=<NllLossBackward0>) lol\n",
            "2.3003674373030663  huu\n",
            "tensor(0.2048, grad_fn=<NllLossBackward0>) lol\n",
            "2.5051375702023506  huu\n",
            "tensor(0.2003, grad_fn=<NllLossBackward0>) lol\n",
            "2.705464906990528  huu\n",
            "tensor(0.1712, grad_fn=<NllLossBackward0>) lol\n",
            "2.8766501918435097  huu\n",
            "tensor(0.2782, grad_fn=<NllLossBackward0>) lol\n",
            "3.154805041849613  huu\n",
            "tensor(0.2216, grad_fn=<NllLossBackward0>) lol\n",
            "3.37636312097311  huu\n",
            "tensor(0.1919, grad_fn=<NllLossBackward0>) lol\n",
            "3.5682752951979637  huu\n",
            "tensor(0.1766, grad_fn=<NllLossBackward0>) lol\n",
            "3.7448907420039177  huu\n",
            "tensor(0.1034, grad_fn=<NllLossBackward0>) lol\n",
            "3.8483124002814293  huu\n",
            "tensor(0.2620, grad_fn=<NllLossBackward0>) lol\n",
            "4.110304944217205  huu\n",
            "tensor(0.2613, grad_fn=<NllLossBackward0>) lol\n",
            "4.3716279193758965  huu\n",
            "tensor(0.1118, grad_fn=<NllLossBackward0>) lol\n",
            "4.4833966717123985  huu\n",
            "tensor(0.2577, grad_fn=<NllLossBackward0>) lol\n",
            "4.741080604493618  huu\n",
            "tensor(0.3621, grad_fn=<NllLossBackward0>) lol\n",
            "5.103171668946743  huu\n",
            "tensor(0.2070, grad_fn=<NllLossBackward0>) lol\n",
            "5.31019251793623  huu\n",
            "tensor(0.2645, grad_fn=<NllLossBackward0>) lol\n",
            "5.574691705405712  huu\n",
            "tensor(0.2245, grad_fn=<NllLossBackward0>) lol\n",
            "5.799169801175594  huu\n",
            "tensor(0.2231, grad_fn=<NllLossBackward0>) lol\n",
            "6.022243373095989  huu\n",
            "tensor(0.2457, grad_fn=<NllLossBackward0>) lol\n",
            "6.267980940639973  huu\n",
            "tensor(0.1038, grad_fn=<NllLossBackward0>) lol\n",
            "6.37180982530117  huu\n",
            "tensor(0.1881, grad_fn=<NllLossBackward0>) lol\n",
            "6.559956610202789  huu\n",
            "tensor(0.3168, grad_fn=<NllLossBackward0>) lol\n",
            "6.876708984375  huu\n",
            "tensor(0.1680, grad_fn=<NllLossBackward0>) lol\n",
            "7.044746965169907  huu\n",
            "tensor(0.2185, grad_fn=<NllLossBackward0>) lol\n",
            "7.263260588049889  huu\n",
            "tensor(0.2109, grad_fn=<NllLossBackward0>) lol\n",
            "7.474144071340561  huu\n",
            "tensor(0.2597, grad_fn=<NllLossBackward0>) lol\n",
            "7.733831346035004  huu\n",
            "tensor(0.1896, grad_fn=<NllLossBackward0>) lol\n",
            "7.923400014638901  huu\n",
            "tensor(0.1686, grad_fn=<NllLossBackward0>) lol\n",
            "8.09199932217598  huu\n",
            "tensor(0.2175, grad_fn=<NllLossBackward0>) lol\n",
            "8.309469535946846  huu\n",
            "tensor(0.3388, grad_fn=<NllLossBackward0>) lol\n",
            "8.648281022906303  huu\n",
            "tensor(0.1235, grad_fn=<NllLossBackward0>) lol\n",
            "8.771794833242893  huu\n",
            "tensor(0.2147, grad_fn=<NllLossBackward0>) lol\n",
            "8.986464761197567  huu\n",
            "tensor(0.2583, grad_fn=<NllLossBackward0>) lol\n",
            "9.244778983294964  huu\n",
            "tensor(0.2473, grad_fn=<NllLossBackward0>) lol\n",
            "9.492105402052402  huu\n",
            "tensor(0.2449, grad_fn=<NllLossBackward0>) lol\n",
            "9.737012289464474  huu\n",
            "tensor(0.3103, grad_fn=<NllLossBackward0>) lol\n",
            "10.04728152602911  huu\n",
            "tensor(0.2417, grad_fn=<NllLossBackward0>) lol\n",
            "10.289004184305668  huu\n",
            "tensor(0.1772, grad_fn=<NllLossBackward0>) lol\n",
            "10.466190122067928  huu\n",
            "tensor(0.1895, grad_fn=<NllLossBackward0>) lol\n",
            "10.655735336244106  huu\n",
            "tensor(0.2610, grad_fn=<NllLossBackward0>) lol\n",
            "10.916719906032085  huu\n",
            "tensor(0.3436, grad_fn=<NllLossBackward0>) lol\n",
            "11.26033204048872  huu\n",
            "tensor(0.2577, grad_fn=<NllLossBackward0>) lol\n",
            "11.518022023141384  huu\n",
            "tensor(0.1308, grad_fn=<NllLossBackward0>) lol\n",
            "11.648803807795048  huu\n",
            "tensor(0.1629, grad_fn=<NllLossBackward0>) lol\n",
            "11.811746396124363  huu\n",
            "tensor(0.3249, grad_fn=<NllLossBackward0>) lol\n",
            "12.136612333357334  huu\n",
            "tensor(0.1922, grad_fn=<NllLossBackward0>) lol\n",
            "12.328845627605915  huu\n",
            "tensor(0.1584, grad_fn=<NllLossBackward0>) lol\n",
            "12.487263314425945  huu\n",
            "tensor(0.2284, grad_fn=<NllLossBackward0>) lol\n",
            "12.71564295142889  huu\n",
            "tensor(0.1247, grad_fn=<NllLossBackward0>) lol\n",
            "12.840328328311443  huu\n",
            "tensor(0.2508, grad_fn=<NllLossBackward0>) lol\n",
            "13.091086857020855  huu\n",
            "tensor(0.2045, grad_fn=<NllLossBackward0>) lol\n",
            "13.295583628118038  huu\n",
            "tensor(0.1322, grad_fn=<NllLossBackward0>) lol\n",
            "13.42774634808302  huu\n",
            "tensor(0.1531, grad_fn=<NllLossBackward0>) lol\n",
            "13.580848895013332  huu\n",
            "tensor(0.2268, grad_fn=<NllLossBackward0>) lol\n",
            "13.807636342942715  huu\n",
            "tensor(0.1068, grad_fn=<NllLossBackward0>) lol\n",
            "13.9144257158041  huu\n",
            "tensor(0.2516, grad_fn=<NllLossBackward0>) lol\n",
            "14.166020467877388  huu\n",
            "tensor(0.1626, grad_fn=<NllLossBackward0>) lol\n",
            "14.328625336289406  huu\n",
            "tensor(0.1477, grad_fn=<NllLossBackward0>) lol\n",
            "14.476369112730026  huu\n",
            "tensor(0.1699, grad_fn=<NllLossBackward0>) lol\n",
            "14.646219804883003  huu\n",
            "tensor(0.1938, grad_fn=<NllLossBackward0>) lol\n",
            "14.840037882328033  huu\n",
            "tensor(0.2981, grad_fn=<NllLossBackward0>) lol\n",
            "15.1381134390831  huu\n",
            "tensor(0.3933, grad_fn=<NllLossBackward0>) lol\n",
            "15.531370997428894  huu\n",
            "tensor(0.2226, grad_fn=<NllLossBackward0>) lol\n",
            "15.75401271879673  huu\n",
            "tensor(0.3127, grad_fn=<NllLossBackward0>) lol\n",
            "16.06668208539486  huu\n",
            "tensor(0.1958, grad_fn=<NllLossBackward0>) lol\n",
            "16.262497827410698  huu\n",
            "tensor(0.1771, grad_fn=<NllLossBackward0>) lol\n",
            "16.439635545015335  huu\n",
            "tensor(0.1697, grad_fn=<NllLossBackward0>) lol\n",
            "16.6093278080225  huu\n",
            "tensor(0.1434, grad_fn=<NllLossBackward0>) lol\n",
            "16.752743989229202  huu\n",
            "tensor(0.2030, grad_fn=<NllLossBackward0>) lol\n",
            "16.955732330679893  huu\n",
            "tensor(0.2147, grad_fn=<NllLossBackward0>) lol\n",
            "17.170460432767868  huu\n",
            "tensor(0.0957, grad_fn=<NllLossBackward0>) lol\n",
            "17.26612113416195  huu\n",
            "epoch : 10/10, recon loss = 0.21056245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop and Validation testing"
      ],
      "metadata": {
        "id": "gK01JNrnoivl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)"
      ],
      "metadata": {
        "id": "awXBdVe1haqH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx29PUYxqyOs",
        "outputId": "013d9d62-ab54-4102-f057-f3acf6b5e6d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7da7fbb4b0a0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(n_total_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWt4JzxBozoI",
        "outputId": "e0ad821c-6fe3-4e8a-8c5a-9a6a02a0a633"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = []\n",
        "l2 = []\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = 0\n",
        "    for i, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        features = features.to(device)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "\n",
        "\n",
        "        # reset the gradients back to zero\n",
        "        # PyTorch accumulates gradients on subsequent backward passes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute reconstructions\n",
        "        outputs = model(features)\n",
        "\n",
        "        # compute training reconstruction loss\n",
        "        train_loss = Loss(outputs, labels)\n",
        "\n",
        "        # compute accumulated gradients\n",
        "        train_loss.backward()\n",
        "\n",
        "        # perform parameter update based on current gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # add the mini-batch training loss to epoch loss\n",
        "        train_loss += train_loss.item()\n",
        "\n",
        "\n",
        "    # compute the epoch training loss\n",
        "    loss = train_loss / len(train_loader)\n",
        "    l1.append(loss)\n",
        "\n",
        "    # display the epoch training loss\n",
        "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch + 1, 10, loss))\n",
        "\n",
        "\n",
        "    min_valid_loss = np.inf\n",
        "    valid_loss = 0.0\n",
        "    model.eval()     # Optional when not using Model Specific layer\n",
        "    for data, labels in val_loader:\n",
        "        # Transfer Data to GPU if available\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = Loss(target,labels)\n",
        "        # Calculate Loss\n",
        "        valid_loss += loss.item()\n",
        "    l2.append(valid_loss / len(val_loader))\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\n \\t\\t Training Loss: { train_loss / len(train_loader)} \\t\\t Validation Loss: { valid_loss / len(val_loader)}')\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "\n",
        "\n",
        "\n",
        "    min_test_loss = np.inf\n",
        "    test_loss = 0.0\n",
        "    model.eval()     # Optional when not using Model Specific layer\n",
        "\n",
        "    correct = 0\n",
        "    for data, labels in test_loader:\n",
        "        # Transfer Data to GPU if available\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = Loss(target,labels)\n",
        "        # Calculate Loss\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        correct += (target.argmax(1) == labels).type(torch.float).sum().item()\n",
        "\n",
        "    l2.append(test_loss / len(test_loader))\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\n \\t\\t Training Loss: { train_loss / len(train_loader)} \\t\\t Validation Loss: { valid_loss / len(val_loader)} \\t\\t Test Loss: {test_loss/len(test_loader)} \\t\\t Accuracy: {((correct/len(test_loader.dataset))*100):>8f}%')\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "        # Saving State Dict\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dszj_TYhpAsf",
        "outputId": "cf876fd4-2e6c-4c7c-ef9e-2b10a7cf2823"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1/10, recon loss = 0.00332869\n",
            "Epoch 1 \n",
            " \t\t Training Loss: 0.0033286912366747856 \t\t Validation Loss: 0.1967395166201251\n",
            "Validation Loss Decreased(inf--->5.508706) \t Saving The Model\n",
            "Epoch 1 \n",
            " \t\t Training Loss: 0.0033286912366747856 \t\t Validation Loss: 0.1967395166201251 \t\t Test Loss: 0.1826850221093212 \t\t Accuracy: 92.983101%\n",
            "epoch : 2/10, recon loss = 0.00351050\n",
            "Epoch 2 \n",
            " \t\t Training Loss: 0.0035105037968605757 \t\t Validation Loss: 0.19866518356970378\n",
            "Validation Loss Decreased(inf--->5.562625) \t Saving The Model\n",
            "Epoch 2 \n",
            " \t\t Training Loss: 0.0035105037968605757 \t\t Validation Loss: 0.19866518356970378 \t\t Test Loss: 0.1849589587322303 \t\t Accuracy: 92.946363%\n",
            "epoch : 3/10, recon loss = 0.00303766\n",
            "Epoch 3 \n",
            " \t\t Training Loss: 0.003037660149857402 \t\t Validation Loss: 0.19622792463217462\n",
            "Validation Loss Decreased(inf--->5.494382) \t Saving The Model\n",
            "Epoch 3 \n",
            " \t\t Training Loss: 0.003037660149857402 \t\t Validation Loss: 0.19622792463217462 \t\t Test Loss: 0.18147033586033753 \t\t Accuracy: 92.946363%\n",
            "epoch : 4/10, recon loss = 0.00314127\n",
            "Epoch 4 \n",
            " \t\t Training Loss: 0.0031412735115736723 \t\t Validation Loss: 0.1974002274551562\n",
            "Validation Loss Decreased(inf--->5.527206) \t Saving The Model\n",
            "Epoch 4 \n",
            " \t\t Training Loss: 0.0031412735115736723 \t\t Validation Loss: 0.1974002274551562 \t\t Test Loss: 0.18178926594555378 \t\t Accuracy: 92.946363%\n",
            "epoch : 5/10, recon loss = 0.00457642\n",
            "Epoch 5 \n",
            " \t\t Training Loss: 0.004576424136757851 \t\t Validation Loss: 0.19693176953920297\n",
            "Validation Loss Decreased(inf--->5.514090) \t Saving The Model\n",
            "Epoch 5 \n",
            " \t\t Training Loss: 0.004576424136757851 \t\t Validation Loss: 0.19693176953920297 \t\t Test Loss: 0.1824033542403153 \t\t Accuracy: 92.836150%\n",
            "epoch : 6/10, recon loss = 0.00488818\n",
            "Epoch 6 \n",
            " \t\t Training Loss: 0.004888184368610382 \t\t Validation Loss: 0.19751945271023683\n",
            "Validation Loss Decreased(inf--->5.530545) \t Saving The Model\n",
            "Epoch 6 \n",
            " \t\t Training Loss: 0.004888184368610382 \t\t Validation Loss: 0.19751945271023683 \t\t Test Loss: 0.18158048604215896 \t\t Accuracy: 92.909625%\n",
            "epoch : 7/10, recon loss = 0.00564183\n",
            "Epoch 7 \n",
            " \t\t Training Loss: 0.005641832947731018 \t\t Validation Loss: 0.1976245048322848\n",
            "Validation Loss Decreased(inf--->5.533486) \t Saving The Model\n",
            "Epoch 7 \n",
            " \t\t Training Loss: 0.005641832947731018 \t\t Validation Loss: 0.1976245048322848 \t\t Test Loss: 0.18244685578559125 \t\t Accuracy: 93.019838%\n",
            "epoch : 8/10, recon loss = 0.00306492\n",
            "Epoch 8 \n",
            " \t\t Training Loss: 0.0030649162363260984 \t\t Validation Loss: 0.1985619908996991\n",
            "Validation Loss Decreased(inf--->5.559736) \t Saving The Model\n",
            "Epoch 8 \n",
            " \t\t Training Loss: 0.0030649162363260984 \t\t Validation Loss: 0.1985619908996991 \t\t Test Loss: 0.1839943788945675 \t\t Accuracy: 92.762675%\n",
            "epoch : 9/10, recon loss = 0.00435638\n",
            "Epoch 9 \n",
            " \t\t Training Loss: 0.004356383811682463 \t\t Validation Loss: 0.19851502749536717\n",
            "Validation Loss Decreased(inf--->5.558421) \t Saving The Model\n",
            "Epoch 9 \n",
            " \t\t Training Loss: 0.004356383811682463 \t\t Validation Loss: 0.19851502749536717 \t\t Test Loss: 0.18179063898112094 \t\t Accuracy: 92.799412%\n",
            "epoch : 10/10, recon loss = 0.00778912\n",
            "Epoch 10 \n",
            " \t\t Training Loss: 0.0077891224063932896 \t\t Validation Loss: 0.19452645177287714\n",
            "Validation Loss Decreased(inf--->5.446741) \t Saving The Model\n",
            "Epoch 10 \n",
            " \t\t Training Loss: 0.0077891224063932896 \t\t Validation Loss: 0.19452645177287714 \t\t Test Loss: 0.1787783285336835 \t\t Accuracy: 92.983101%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming l1 is your list or array of loss values\n",
        "l1_np = [item.detach().numpy() for item in l1]\n",
        "\n",
        "plt.plot(l1_np)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"training loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "L_FEQjpdXY4e",
        "outputId": "13c1959e-81f6-497b-d709-e37aa2823b57"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming l1 is your list or array of loss values\n",
        "# l2_np = [item.detach().numpy() for item in l2]\n",
        "\n",
        "plt.plot(l2)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"Validation loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "--wk7iNyXm_1",
        "outputId": "1c63d0fe-24bf-45fb-dc7a-69414a30494a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}